---
title: "[ML] í˜¼ê³µë¨¸ë‹ 2í¸ k-Nearest Neighbors Regressor(k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€)"
excerpt: "k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€ ì•Œê³ ë¦¬ì¦˜ê³¼ ê³¼ëŒ€,ê³¼ì†Œ ì í•©, ëª¨ë¸ ê·œì œ ëŒ€í•´ ì•Œì•„ë³´ì"
toc: true
toc_label: "ëª©ì°¨"
toc_sticky: true

tags: [k-ìµœê·¼ì ‘ì´ì›ƒ, ML]

published: true

categories:
  - ML

date: 2022-11-28 00:30:30
last_modified_at: 2022-11-28 00:30:30
---

<div class="notice--primary" markdown="1">
ğŸ’¡ â€˜í˜¼ì ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹(ë°•í•´ì„  ì €)â€™ ì±…ì„ ì½ê³  ê³µë¶€í•œ ë‚´ìš©ì„ ìš”ì•½í•œ í˜ì´ì§€ì…ë‹ˆë‹¤.<br>
ì±…ì— ë‚˜ì˜¤ëŠ” ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì“°ì§€ ì•Šê³ , ì½”ë“œì™€ íŒŒë¼ë¯¸í„°ë¥¼ ë³€í˜•í•˜ê³  ì¡°ì •í•´ê°€ë©° ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ë©° ê³µë¶€í–ˆìŠµë‹ˆë‹¤.
</div>

<br>
## 03 íšŒê·€ ì•Œê³ ë¦¬ì¦˜ê³¼ ëª¨ë¸ ê·œì œ

---

### **03-1 k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€**

- **k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€**: ì˜ˆì¸¡í•˜ë ¤ëŠ” ìƒ˜í”Œì— ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ kê°œë¥¼ ì„ íƒí•˜ê³  ì´ ìˆ˜ì¹˜ë“¤ì˜ í‰ê· ê°’ì„ ì˜ˆì¸¡ íƒ€ê¹ƒê°’ìœ¼ë¡œ ì¶”ì •í•œë‹¤.

ë†ì–´ì˜ ê¸¸ì´, ë¬´ê²Œ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì–´ìˆì„ ë•Œ, ì˜ˆì¸¡í•˜ë ¤ëŠ” ë†ì–´ì˜ ê¸¸ì´ê°€ ì£¼ì–´ì§€ë©´ ë¬´ê²Œê°€ ì–¼ë§ˆë‚˜ ë‚˜ê°ˆì§€ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ê³ ì í•œë‹¤.

```python
#ë°ì´í„° ì¤€ë¹„
import numpy as np
perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```

```python
#í›ˆë ¨ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ì„¸íŠ¸ë¡œ ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

#í˜„ì¬ 1ì°¨ì› ë°°ì—´ì¸ ë°ì´í„°ë“¤ì„ 2ì°¨ì› ë°°ì—´ë¡œ ë§Œë“œëŠ” ì‘ì—…
train_input = train_input.reshape(-1,1) #reshape()ë©”ì„œë“œëŠ” ë°°ì—´ í¬ê¸°ë¥¼ ë°”ê¿”ì¤Œ.
test_input = test_input.reshape(-1,1) #-1ì„ ì§€ì •í•˜ë©´ ë‚˜ë¨¸ì§€ ì›ì†Œ ê°œìˆ˜ë¡œ ëª¨ë‘ ì±„ìš°ë¼ëŠ” ì˜ë¯¸

from sklearn.neighbors import KNeighborsRegressor #k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ í´ë˜ìŠ¤
knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
print(knr.score(test_input, test_target)) #ì´ ë•Œ ì¶œë ¥ë˜ëŠ” ì ìˆ˜ê°€ ë°”ë¡œ ê²°ì •ê³„ìˆ˜(R^2)
```

<div align= 'center'>

<img src="https://user-images.githubusercontent.com/115082062/204143238-0e4e4ac8-245d-458b-885a-fad5deac7bf2.png">

</div>


```python
#ê²°ì •ê³„ìˆ˜ê°€ ì•„ë‹Œ íƒ€ê¹ƒê³¼ ì˜ˆì¸¡ì˜ ì ˆëŒ“ê°’ ì˜¤ì°¨ë¥¼ í‰ê· í•˜ëŠ” ë°©ì‹
from sklearn.metrics import mean_absolute_error
test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)
print(mae)
```

- **ê³¼ëŒ€ì í•©(overfitting)**: í›ˆë ¨ì„¸íŠ¸ì—ì„œ ì ìˆ˜ê°€ ì•„ì£¼ ì¢‹ì•˜ëŠ”ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œëŠ” ì ìˆ˜ê°€ êµ‰ì¥íˆ ë‚˜ìœ ê²½ìš°
- **ê³¼ì†Œì í•©(underfitting)**: í›ˆë ¨ì„¸íŠ¸ë³´ë‹¤ í…ŒìŠ¤íŠ¸ì„¸íŠ¸ì˜ ì ìˆ˜ê°€ ë†’ê±°ë‚˜ ë‘˜ ë‹¤ ë„ˆë¬´ ë‚®ì€ ê²½ìš°
 

```python
knr.n_neighbors = 3 #ê¸°ë³¸ê°’ 5ë³´ë‹¤ ì´ì›ƒì˜ ê°œìˆ˜ë¥¼ ì¤„ì´ë©´ êµ­ì§€ì ì¸ íŒ¨í„´ì— ë¯¼ê°í•´ì§
#ì¦‰ overfittingì˜ ì—¬ì§€ê°€ ì»¤ì§
knr.fit(train_input, train_target)
print(knr.score(train_input, train_target)) #0.9804899950518966
print(knr.score(test_input, test_target)) #0.974645996398761
#í…ŒìŠ¤íŠ¸ì„¸íŠ¸ì˜ ì ìˆ˜ê°€ ë” ë‚®ê³  ë‘˜ ê°„ì˜ ì°¨ì´ë„ í¬ì§€ ì•Šìœ¼ë‹ˆ ê³¼ì†Œì í•©ë„ ê³¼ëŒ€ì í•©ë„ ì•„ë‹˜!
```
<div align= 'center'>

<img src="https://user-images.githubusercontent.com/115082062/204143293-d11c60b4-8834-4f83-a50b-a0a140711573.png">

</div>

n_neighbors ë§¤ê°œë³€ìˆ˜ë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ ë°ì´í„° ì „ë°˜ì˜ ê²½í–¥ì„ ë”°ë¼ê°€ê³ , ì¤„ì¼ìˆ˜ë¡ í›ˆë ¨ì„¸íŠ¸ ìƒ˜í”Œì— ì² ì €í•˜ê²Œ ë¨–ì¶°ì§€ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤.

**03-1 í•µì‹¬ í‚¤ì›Œë“œ**

- **k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€**: ì˜ˆì¸¡í•˜ë ¤ëŠ” ìƒ˜í”Œì— ê°€ì¥ ê°€ê¹Œìš´ ìƒ˜í”Œ kê°œë¥¼ ì„ íƒí•˜ê³  ì´ ìˆ˜ì¹˜ë“¤ì˜ í‰ê· ê°’ì„ ì˜ˆì¸¡ íƒ€ê¹ƒê°’ìœ¼ë¡œ ì¶”ì •
- **ê²°ì •ê³„ìˆ˜**: ëŒ€í‘œì ì¸ íšŒê·€ ë¬¸ì œì˜ ì„±ëŠ¥ ì¸¡ì • ë„êµ¬. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ë‹¤.
- ê³¼ëŒ€ì í•©: í›ˆë ¨ì„¸íŠ¸ì—ì„œ ì ìˆ˜ê°€ ì•„ì£¼ ì¢‹ì•˜ëŠ”ë° í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œëŠ” ì ìˆ˜ê°€ êµ‰ì¥íˆ ë‚˜ìœ ê²½ìš°
- ê³¼ì†Œì í•©: í›ˆë ¨ì„¸íŠ¸ë³´ë‹¤ í…ŒìŠ¤íŠ¸ì„¸íŠ¸ì˜ ì ìˆ˜ê°€ ë†’ê±°ë‚˜ ë‘˜ ë‹¤ ë„ˆë¬´ ë‚®ì€ ê²½ìš°

**03-1 í•µì‹¬ íŒ¨í‚¤ì§€ì™€ í•¨ìˆ˜**

- scikit-learn
    - `KNeighborsRegressor`: k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€ë¥¼ êµ¬í˜„í•˜ëŠ” ì‚¬ì´í‚·ëŸ° í´ë˜ìŠ¤. n_neighbors ë§¤ê°œë³€ìˆ˜ë¡œ ì´ì›ƒì˜ ê°œìˆ˜ë¥¼ ì§€ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 5
    - `mean_absolute_error()`: íšŒê·€ëª¨ë¸ì˜ í‰ê·  ì ˆëŒ“ê°’ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•œë‹¤.
- numpy
    - **`reshape()`**: ë°°ì—´ì˜ í¬ê¸°ë¥¼ ë°”ê¾¸ëŠ” ë©”ì„œë“œ.

---

<br>

### **03-2 ì„ í˜• íšŒê·€**

k-ìµœê·¼ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì˜ í•œê³„

```python
print(knr.predict([[50]])) 
#ì‹¤ì œë¡œ ì €ìš¸ì— ì¬ë³´ë‹ˆ 1500gì¸ë°, ìµœê·¼ì ‘íšŒê·€ëª¨ë¸ì—ì„  1033.33gìœ¼ë¡œ ì˜ˆì¸¡í•¨

import matplotlib.pyplot as plt
distnces, indexes = knr.kneighbors([[50]])
plt.scatter(train_input, train_target)
plt.scatter(train_input[indexes], train_target[indexes], marker='D')
plt.scatter(50,1033,marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
#ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ë¬´ê²Œê°€ ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì€ ë‹¹ì—°í•œë°, ê¸¸ì´ê°€ ë” ê¸¸ì–´ì ¸ë„ ìµœê·¼ì ‘í•˜ëŠ” ê²ƒë“¤ì˜ ë¬´ê²ŒëŠ” ì¼ì •í•˜ê¸°ì—, ì˜ˆì¸¡ê°’ì´ ì»¤ì§€ì§€ ì•ŠëŠ”ë‹¤. 
#ì˜ˆë¥¼ ë“¤ì–´ ê¸¸ì´ê°€ 100cmë¼ë„ ë¬´ê²ŒëŠ” ì—¬ì „íˆ 1033gì¼ ê²ƒì´ë¼ëŠ” ë§ì´ë‹¤.
```

<div align= 'center'>

<img src="https://user-images.githubusercontent.com/115082062/204143467-950b9769-e892-41d2-b628-e923c427a489.png">

</div>

ì‚°ì ë„ë¡œ ë³´ë©´ í™•ì—°í•œ k-ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€ì˜ í•œê³„ì . ì´ëŸ° ì‹ì´ë©´ ë†ì–´ê°€ ì•„ë¬´ë¦¬ ì»¤ë„ ë¬´ê²Œê°€ ë” ëŠ˜ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤.

<br>

- **ì„ í˜•íšŒê·€(linear regression)**: íŠ¹ì„±ì´ í•˜ë‚˜ì¸ ê²½ìš° ê·¸ íŠ¹ì„±ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ì§ì„ ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜
>

```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(train_input, train_target)
print(lr.predict([[50]])) #1241.83 ì¶œë ¥
print(lr.coef_,lr.intercept_) #coefì—ëŠ” ê¸°ìš¸ê¸° ê³„ìˆ˜ê°€, interceptì—ëŠ” ì ˆí¸ì´ ì €ì¥ë˜ì–´ ìˆë‹¤.

plt.scatter(train_input, train_target)
plt.plot([15,50],[15*lr.coef_ + lr.intercept_, 50*lr.coef_ + lr.intercept_])
plt.scatter(50, 1241.8, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<div align= 'center'>

<img src="https://user-images.githubusercontent.com/115082062/204143541-4a0912cc-b969-416e-a65f-79e4576b3a08.png">


</div>

ì•ì„œ ì§€ì ëœ ìµœê·¼ì ‘ ì´ì›ƒ íšŒê·€ì˜ ë¬¸ì œì ì€ í•´ì†Œê°€ ë˜ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë­”ê°€ ì´ìƒí•œ ë¶€ë¶„ì´ ìˆë‹¤.

```python
print(lr.score(train_input, train_target))
print(lr.score(test_input, test_target))
#ë‘ ê²°ì •ê³„ìˆ˜(R_squared) ëª¨ë‘ ë†’ì€ í¸ì´ ì•„ë‹ˆë¼ ê³¼ì†Œì í•© ëœ ë“¯í•˜ë‹¤.
#ë˜í•œ, ê·¸ë˜í”„ ì¢Œì¸¡í•˜ë‹¨ì„ ë³´ë©´ weightê°€ 0ì•„ë˜ë¡œ ë‚´ë ¤ê°€ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, ë¬´ê²Œê°€ ìŒìˆ˜ì¼ ìˆ˜ëŠ” ì—†ìœ¼ë‹ˆ ë§ì´ ì•ˆëœë‹¤.
```

ìƒê¸°í•œ ë‘ ê°€ì§€ ë¬¸ì œì ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ ì„ í˜• íšŒê·€ê°€ ì•„ë‹ˆë¼ **ë‹¤í•­íšŒê·€(polynomial regression)** ëª¨ë¸ì„ ì ìš©í•´ì•¼ í•œë‹¤. ë‹¤ì‹œë§í•´, ìµœì ì˜ ì§ì„ ì„ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìµœì ì˜ ê³¡ì„ ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.

```python
#2ì°¨ ë°©ì •ì‹ ê·¸ë˜í”„ë¥¼ ë§Œë“œë ¤ë©´ ê¸¸ì´ë¥¼ ì œê³±í•œ í•­ì´ train_inputì— ì¶”ê°€ë˜ì–´ì•¼ í•œë‹¤!
train_poly = np.column_stack((train_input**2, train_input))
test_poly = np.column_stack((test_input**2, test_input))
lr.fit(train_poly,train_target)
print(lr.predict([[50**2,50]]))
print(lr.coef_, lr.intercept_) #[  1.01433211 -21.55792498] 116.05021078278276
# ì¦‰, ë¬´ê²Œ = 1.01*ê¸¸ì´^2 - 21.6*ê¸¸ì´ +116.05ë¼ëŠ” íšŒê·€ì‹ì„ ë§Œë“  ê²ƒ.
```

```python
point = np.arange(15,50) #êµ¬ê°„ë³„ ì§ì„ ì„ ê·¸ë¦¬ê¸° ìœ„í•´ 15ì—ì„œ 49ê¹Œì§€ ì •ìˆ˜ ë°°ì—´ì„ ë§Œë“¦.
plt.scatter(train_input, train_target)
plt.plot(point, point**2*1.01 - point*21.6 + 116.05) #êµ¬ê°„ë³„ë¡œ ì§ì„  ì—¬ëŸ¬ê°œë¥¼ í•©ì³ ê³¡ì„ ì²˜ëŸ¼ ë§Œë“  ê²ƒ.
plt.scatter(50, 1574, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

print(lr.score(train_poly, train_target))
print(lr.score(test_poly, test_target))
```

<div align= 'center'>

<img src="https://user-images.githubusercontent.com/115082062/204143569-a27e9c87-7c6c-4728-a617-64aff0e99ebd.png">

</div>

ì„ í˜•íšŒê·€ë³´ë‹¤ ë” ì í•©í•œ ê·¸ë˜í”„ê°€ ê·¸ë ¤ì¡Œë‹¤.

**03-2 í•µì‹¬ í‚¤ì›Œë“œ**

- ì„ í˜• íšŒê·€: íŠ¹ì„±ê³¼ íƒ€ê¹ƒ ì‚¬ì´ ê´€ê³„ë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ì„ í˜• ë°©ì •ì‹ì„ ì°¾ëŠ” ëª¨ë¸
- ëª¨ë¸ íŒŒë¼ë¯¸í„°: ì„ í˜• íšŒê·€ê°€ ì°¾ì€ ê°€ì¤‘ì¹˜ì²˜ëŸ¼ ëª¨ë¸ì´ íŠ¹ì„±ì—ì„œ í•™ìŠµí•œ íŒŒë¼ë¯¸í„°ë¥¼ ì˜ë¯¸
- ë‹¤í•­ íšŒê·€: ë‹¤í•­ì‹ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì„±ê³¼ íƒ€ê¹ƒ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ„

**03-2 í•µì‹¬ íŒ¨í‚¤ì§€ì™€ í•¨ìˆ˜**

- scikit-learn
    - `LinearRegression`ì€ ì‚¬ì´í‚·ëŸ°ì˜ ì„ í˜• íšŒê·€ í´ë˜ìŠ¤ì´ë‹¤.  `coef_` ì†ì„±ì€ ê³„ìˆ˜ì˜ ë°°ì—´ì´ê³ , `intercept_` ì†ì„±ì€ ì ˆí¸ì´ë‹¤.

---

<br>

### **03-3 íŠ¹ì„± ê³µí•™**

ì§€ê¸ˆê¹Œì§€ëŠ” í•˜ë‚˜ì˜ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì—¬ ì„ í˜•íšŒê·€ëª¨ë¸ì„ í›ˆë ¨ì‹œì¼°ë‹¤. ì´ë²ˆì—ëŠ” ì—¬ëŸ¬ ê°œì˜ íŠ¹ì„±ì„ ì‚¬ìš©í•œ **ë‹¤ì¤‘íšŒê·€(multiple regression)**ë¥¼ ì‹¤í–‰í•´ë³¸ë‹¤. íŠ¹ì„±ì´ 2ê°œë©´ ì„ í˜• íšŒê·€ëŠ” í‰ë©´ì„ í•™ìŠµí•œë‹¤.

- **íŠ¹ì„± ê³µí•™(feature engineering)**: ê¸°ì¡´ì˜ íŠ¹ì„±ì„ ê°€ê³µí•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ë½‘ì•„ë‚´ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ â€˜ë†ì–´ê¸¸ì´ * ë†ì–´ë†’ì´â€™ë¡œ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ë§Œë“œëŠ” ë°©ì‹

- **íŒë‹¤ìŠ¤(pandas)**: ì¸í„°ë„·ì—ì„œ ë°ì´í„°ë¥¼ ë°”ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ë°ì´í„°í”„ë ˆì„ì€ íŒë‹¤ìŠ¤ì˜ í•µì‹¬ ë°ì´í„° êµ¬ì¡°ë¡œ, ë„˜íŒŒì´ ë°°ì—´ê³¼ ë¹„ìŠ·í•˜ê²Œ ë‹¤ì°¨ì› ë°°ì—´ì„ ë‹¤ë£° ìˆ˜ ìˆê³  í›¨ì”¬ ë§ì€ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤.

```python
import pandas as pd
df = pd.read_csv('https://bit.ly/perch_csv_data')
perch_full = df.to_numpy()
```

```python

perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
```

ì‚¬ì´í‚·ëŸ°ì€ íŠ¹ì„±ì„ ë§Œë“¤ê±°ë‚˜ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ë¥¼ ì œê³µí•˜ëŠ”ë°, ì´ëŸ¬í•œ í´ë˜ìŠ¤ë¥¼ **ë³€í™˜ê¸°(transformer)** ë¼ê³  í•œë‹¤.

```python
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(include_bias = False) #ì ˆí¸ì€ ì œì™¸í•œë‹¤ëŠ” ì˜ë¯¸
poly.fit(train_input) #í›ˆë ¨ì„¸íŠ¸ì˜ ìƒ˜í”Œë“¤ì„ ì„œë¡œ ê³±í•œ ê°’ê³¼ ì œê³±í•œ ê°’ìœ¼ë¡œ íŠ¹ì„±ë“¤ì„ í›ˆë ¨
train_poly = poly.transform(train_input) #í›ˆë ¨í•œ ê°’ì„ í† ëŒ€ë¡œ ë³€í™˜
poly.get_feature_names() # ê°ê°ì˜ íŠ¹ì„±ë“¤ì´ ì–´ë–¤ ì¡°í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒëŠ”ì§€ ì´ë¦„ì„ í‘œì‹œ
test_poly = poly.transform(test_input)
poly.get_feature_names()
```

```python
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target)) #0.990318. ë§¤ìš° ë†’ì€ ì ìˆ˜
print(lr.score(test_poly, test_target)) #0.971455
```

```python
#5ì œê³±ê¹Œì§€ ëŠ˜ë ¤ë³´ë©´ ì–´ë–¨ê¹Œ?
poly = PolynomialFeatures(degree=5, include_bias = False) 
#degreeë§¤ê°œë³€ìˆ˜ë¡œ ê³ ì°¨í•­ì˜ ìµœëŒ€ ì°¨ìˆ˜ë¥¼ ì§€ì •
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)

lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target)) #0.999999
print(lr.score(test_poly, test_target)) #-144.405
#ì¦‰, íŠ¹ì„±ì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ í˜•í¸ì—†ëŠ” overfittingì´ ëœë‹¤.
```

**03-3 í•µì‹¬ í‚¤ì›Œë“œ**

- ë‹¤ì¤‘ íšŒê·€(mutiple regression): ì—¬ëŸ¬ ê°œì˜ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ëŠ” íšŒê·€ ëª¨ë¸
- íŠ¹ì„± ê³µí•™: ì£¼ì–´ì§„ íŠ¹ì„±ì„ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ë§Œë“œëŠ” ì¼ë ¨ì˜ ì‘ì—…

**03-3 í•µì‹¬ íŒ¨í‚¤ì§€ì™€ í•¨ìˆ˜**

- **pandas**
    - `read_csv()`ëŠ” csvíŒŒì¼ì„ íŒë‹¤ìŠ¤ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.
- scikit-learn
    - **`PolynomialFeatures`**ëŠ” ì£¼ì–´ì§„ íŠ¹ì„±ì„ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ë§Œë“œëŠ” ë³€í™˜ê¸° í´ë˜ìŠ¤ì´ë‹¤. `include_bias`ê°€ Falseì´ë©´ ì ˆí¸ì„ ìœ„í•œ íŠ¹ì„±ì„ ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤. `degree`ëŠ” ìµœê³  ì°¨ìˆ˜ë¥¼ ì§€ì •í•˜ê³  ê¸°ë³¸ê°’ì€ 2ì´ë‹¤.

---

<br>

### **03-4 ê·œì œ**

- **ê·œì œ(Regularization)**: ëª¨ë¸ì´ ê³¼ëŒ€ì í•©ë˜ì§€ ì•Šë„ë¡ ë§Œë“œëŠ” ì‘ì—…ì´ë‹¤. ì„ í˜• íšŒê·€ ëª¨ë¸ì˜ ê²½ìš° íŠ¹ì„±ì— ê³±í•´ì§€ëŠ” ê³„ìˆ˜ì˜ í¬ê¸°ë¥¼ ì‘ê²Œ ë§Œë“œëŠ” ê³¼ì •ì´ë‹¤. 

íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ì´ ì •ê·œí™”ë˜ì§€ ì•Šìœ¼ë©´ ê³±í•´ì§€ëŠ” ê³„ìˆ˜ ê°’ì—ë„ ì°¨ì´ê°€ ë‚œë‹¤. ë§Œì•½ ì„ í˜• íšŒê·€ ëª¨ë¸ì— ê·œì œë¥¼ ì ìš©í•  ë•Œ ê³„ìˆ˜ ê°’ì˜ í¬ê¸°ê°€ ì„œë¡œ ë§ì´ ë‹¤ë¥´ë©´ ê³µì •í•˜ê²Œ ê·œì œë˜ì§€ ì•Šì„ í…Œë‹ˆ, ë¨¼ì € ì •ê·œí™”ë¥¼ í•´ì¤˜ì•¼ í•œë‹¤.

```python
from sklearn.preprocessing import StandardScaler #í‘œì¤€ì ìˆ˜ë¡œ ë°”ê¿”ì£¼ëŠ” ë³€í™˜ê¸° í´ë˜ìŠ¤
ss = StandardScaler()
ss.fit(train_poly)
train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

- **ë¦¿ì§€(ridge)**: ê³„ìˆ˜ë¥¼ ì œê³±í•œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·œì œí•˜ëŠ” ê²ƒ 

```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target)) #0.9896. ì™„ë²½ì— ê°€ê¹Œìš´ ì ìˆ˜ì—ì„œ ì‚´ì§ í•˜ë½
print(ridge.score(test_scaled, test_target)) #0.9790. ë‹¤ì‹œ ì •ìƒìœ¼ë¡œ ëŒì•„ì˜´. ê·œì œê°€ ì˜ ëë‹¤ëŠ” ì˜ë¯¸.
```

ê·œì œë¥¼ í•  ë•Œì— ê·œì œì˜ ì–‘ì„ ì¡°ì ˆí•˜ëŠ” **alpha** ë§¤ê°œë³€ìˆ˜ê°€ ìˆë‹¤. ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜€ë‹¤ë©´, ì´ë ‡ê²Œ ì‚¬ëŒì´ ì§ì ‘ ì§€ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” **í•˜ì´í¼ íŒŒë¼ë¯¸í„°**ë¼ê³  í•œë‹¤.

```python
#ì ì ˆí•œ alpha ê°’ì„ ì°¾ê¸° ìœ„í•œ ë¹„êµ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
import matplotlib.pyplot as plt
train_score=[]
test_score=[]
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
  ridge = Ridge(alpha=alpha)
  ridge.fit(train_scaled, train_target)
  train_score.append(ridge.score(train_scaled, train_target))
  test_score.append(ridge.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score)
plt.plot(np.log10(alpha_list), test_score)
plt.xlabel('alpha')
plt.ylabel('R_squared')
plt.show()
```

<div align= 'center'>

<img src="https://user-images.githubusercontent.com/115082062/204143669-36e8484e-8409-4713-9649-5a95ac32e731.png">

</div>

íŒŒë€ìƒ‰ì´ í›ˆë ¨ì„¸íŠ¸, ì£¼í™©ìƒ‰ì´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì´ë‹¤.

ì ì ˆí•œ alphaê°’ì€ ë‘ ê·¸ë˜í”„ê°€ ì œì¼ ê°€ê¹ê³  í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ -1, ì¦‰ 10^-1=0.1ë¡œ ë³´ì¸ë‹¤.

```python
ridge=Ridge(alpha=0.1)
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target)) #0.9903
print(ridge.score(test_scaled, test_target)) #0.9827
#alphaë¥¼ 0.1ë¡œ ì„¤ì •í•˜ë‹ˆ ê³¼ëŒ€ì í•©ê³¼ ê³¼ì†Œì í•© ì‚¬ì´ì˜ ê· í˜•ì„ ì˜ ì°¾ìŒ
```

- **ë¼ì˜(lasso)**: ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·œì œí•˜ëŠ” ê²ƒ 

```python
#Ridgeí´ë˜ìŠ¤ë¥¼ Lassoí´ë˜ìŠ¤ë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ë©´ ë¨.
from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```

**03-4 í•µì‹¬ í‚¤ì›Œë“œ**

- **ë¦¿ì§€**: ê³„ìˆ˜ë¥¼ ì œê³±í•œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·œì œí•˜ëŠ” ê²ƒ. íš¨ê³¼ê°€ ì¢‹ì•„ ë„ë¦¬ ì‚¬ìš©ë¨
- **ë¼ì˜**: ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·œì œí•˜ëŠ” ê²ƒ. ë¦¿ì§€ì™€ ë‹¬ë¦¬ ê³„ìˆ˜ë¥¼ ì•„ì˜ˆ 0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ë„ ìˆìŒ
- í•˜ì´í¼íŒŒë¼ë¯¸í„°: ì•Œê³ ë¦¬ì¦˜ì´ í•™ìŠµí•˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹ˆë¼ ì‚¬ëŒì´ ì‚¬ì „ì— ì§€ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°

**03-4  í•µì‹¬ íŒ¨í‚¤ì§€ì™€ í•¨ìˆ˜**

- scikit-learn
    - **`Ridge`**ëŠ” ë¦¿ì§€ íšŒê·€ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜. alphaë§¤ê°œë³€ìˆ˜ë¡œ ê·œì œ ê°•ë„ë¥¼ ì¡°ì •í•˜ê³ , ê¸°ë³¸ê°’ì€ 1ì´ë‹¤.
    - **`Lasso`**ëŠ” ë¼ì˜ íšŒê·€ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜. ì´ í´ë˜ìŠ¤ëŠ” ìµœì ì˜ ëª¨ë¸ì„ ì°¾ê¸° ìœ„í•´ ì¢Œí‘œì¶•ì„ ë”°ë¼ ìµœì ì„ ìˆ˜í–‰í•´ê°€ëŠ” ì¢Œí‘œ í•˜ê°•ë²•(coordinate descent)ì„ ì‚¬ìš©í•œë‹¤.

<br>
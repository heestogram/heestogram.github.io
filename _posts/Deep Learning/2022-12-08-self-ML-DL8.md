---
title: "[ML] í˜¼ê³µë¨¸ë‹ 8í¸ RNN, LSTM, GRU"
excerpt: "í…ìŠ¤íŠ¸ë‚˜ ì‹œê³„ì—´ ë°ì´í„° ê°™ì€ ìˆœì°¨ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ RNN, LSTM, GRU ì‹ ê²½ë§ì„ ì‚¬ìš©í•´ë³´ì"
toc: true
toc_label: "ëª©ì°¨"
toc_sticky: true

tags: [Deep Learning, RNN, LSTM, GRU]

published: true

categories:
  - DL

date: 2022-12-08 15:30:00
last_modified_at: 2022-12-08 15:30:00
---

<br>

<div class="notice--primary" markdown="1">
ğŸ’¡ â€˜í˜¼ì ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹(ë°•í•´ì„  ì €)â€™ ì±…ì„ ì½ê³  ê³µë¶€í•œ ë‚´ìš©ì„ ìš”ì•½í•œ í˜ì´ì§€ì…ë‹ˆë‹¤.<br>
ì±…ì— ë‚˜ì˜¤ëŠ” ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì“°ì§€ ì•Šê³ , ì½”ë“œì™€ íŒŒë¼ë¯¸í„°ë¥¼ ë³€í˜•í•˜ê³  ì¡°ì •í•´ê°€ë©° ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ë©° ê³µë¶€í–ˆìŠµë‹ˆë‹¤.
</div>

<br>

## 09 í…ìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì¸ê³µ ì‹ ê²½ë§

### **09-1 ìˆœì°¨ ë°ì´í„°ì™€ ìˆœí™˜ ì‹ ê²½ë§**

**ìˆœì°¨ ë°ì´í„°(sequential data)**ëŠ” **í…ìŠ¤íŠ¸**ë‚˜ **ì‹œê³„ì—´ ë°ì´í„°(time series data)**ì™€ ê°™ì´ ìˆœì„œì— ì˜ë¯¸ê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ë§í•œë‹¤. ìˆœì°¨ ë°ì´í„°ì˜ ìˆœì„œëŠ” ë’¤ì„ì´ë©´ ì•ˆëœë‹¤. ì¦‰ ìˆœì°¨ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•ŒëŠ” ì´ì „ì— ì…ë ¥í•œ ë°ì´í„°ë¥¼ ê¸°ì–µí•˜ëŠ” ê¸°ëŠ¥ì´ í•„ìš”í•˜ë‹¤.

ì™„ì „ ì—°ê²° ì‹ ê²½ë§ì´ë‚˜ í•©ì„±ê³± ì‹ ê²½ë§ì€ ì´ëŸ° ê¸°ì–µ ì¥ì¹˜ê°€ ì—†ë‹¤. ì´ë ‡ê²Œ ë°ì´í„°ì˜ íë¦„ì´ ì•ìœ¼ë¡œë§Œ ì „ë‹¬ë˜ëŠ” ì‹ ê²½ë§ì„ **í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§(FFNN)**ì´ë¼ê³  í•œë‹¤.

ë°˜ë©´ ì´ì „ì— ì²˜ë¦¬í•œ ìƒ˜í”Œì„ ì¬ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ì„ **ìˆœí™˜ ì‹ ê²½ë§(recurrent neural network, RNN)**ì´ë¼ê³  í•œë‹¤. ì´ëŠ” ì™„ì „ ì—°ê²° ì‹ ê²½ë§ì— ìˆœí™˜í•˜ëŠ” ê³ ë¦¬ í•˜ë‚˜ë§Œ ì¶”ê°€í•œ í˜•ìƒì´ë‹¤.

A,B,Cë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ìˆœí™˜ ì‹ ê²½ë§ì´ ìˆë‹¤ê³  ê°€ì •í•˜ì. ì²«ë²ˆì§¸ ìƒ˜í”Œ Aë¥¼ ì²˜ë¦¬í•˜ê³  ë‚œ ì¶œë ¥ OAê°€ ë‹¤ì‹œ ë‰´ëŸ°ìœ¼ë¡œ ëŒì•„ê°„ë‹¤. ê·¸ ë‹¤ìŒ ìƒ˜í”Œ Bë¥¼ ì²˜ë¦¬í•  ë• OAì™€ Bê°€ í•¨ê»˜ ì²˜ë¦¬ë˜ì–´ OBê°€ ë§Œë“¤ì–´ì§„ë‹¤. ì´ ë•Œ OBì—” Aì˜ ì •ë³´ê°€ ì¼ì •ëŸ‰ í¬í•¨ë˜ì–´ ìˆë‹¤. ê·¸ ë‹¤ìŒ ìƒ˜í”Œ Cë¥¼ ì²˜ë¦¬í•  ë• OBê°€ í•¨ê»˜ ì“°ì¸ë‹¤. ê·¸ë ‡ê²Œ ë§Œë“¤ì–´ì§„ OCì—ëŠ” Bì™€ Aì˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ìˆë‹¤. ë¬¼ë¡  ì§ì „ì— ì“°ì¸ Bê°€ Aë³´ë‹¨ ì •ë³´ëŸ‰ì´ ë§ë‹¤.

ì´ ë•Œ ìƒ˜í”Œì„ ì²˜ë¦¬í•˜ëŠ” í•œ ë‹¨ê³„ë¥¼ **íƒ€ì„ìŠ¤í…(time step)**ì´ë¼ê³  í•œë‹¤. íƒ€ì„ìŠ¤í…ì´ ì˜¤ë˜ë ìˆ˜ë¡ ìˆœí™˜ë˜ëŠ” ì •ë³´ëŠ” í¬ë¯¸í•´ì§„ë‹¤.

ìˆœí™˜ ì‹ ê²½ë§ì—ì„œëŠ” ì¸µì„ ì…€(cell)ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ì…€ì˜ ì¶œë ¥ì€ **ì€ë‹‰ ìƒíƒœ(hidden state)**ë¼ê³  ë¶€ë¥¸ë‹¤.

ì€ë‹‰ì¸µì˜ í™œì„±í™”í•¨ìˆ˜ë¡œëŠ” í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜ê°€ ì‚¬ìš©ëœë‹¤. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ ìœ ì‚¬í•˜ì§€ë§Œ, -1~1ì˜ ë²”ìœ„ë¥¼ ê°–ëŠ”ë‹¤.

<img src="https://user-images.githubusercontent.com/115082062/206373563-ecc1eee9-5d04-4b0f-86f5-cc4ce41735f5.png">

íŒŒë€ìƒ‰ì´ ì‹œê·¸ëª¨ì´ë“œ, ë¹¨ê°„ìƒ‰ì´ í•˜ì´í¼ë³¼ë¦­ íƒ„ì  íŠ¸ í•¨ìˆ˜.

ìˆœí™˜ ì‹ ê²½ë§ì˜ ë‰´ëŸ°ì€ ê°€ì¤‘ì¹˜ê°€ í•˜ë‚˜ ë” ìˆëŠ”ë°, ë°”ë¡œ ì´ì „ íƒ€ì„ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœì— ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜ì´ë‹¤.

ìˆœí™˜ì¸µì€ ê¸°ë³¸ì ìœ¼ë¡œ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœë§Œ ì¶œë ¥ìœ¼ë¡œ ë‚´ë³´ë‚¸ë‹¤.

í•©ì„±ê³± ì‹ ê²½ë§ê³¼ ë‹¤ë¥¸ ì ì€ ë§ˆì§€ë§‰ ì…€ì˜ ì¶œë ¥ì´ 1ì°¨ì›ì´ê¸° ë•Œë¬¸ì— êµ³ì´ `Flatten` í´ë˜ìŠ¤ë¡œ í¼ì¹  í•„ìš”ê°€ ì—†ê³ , ì…€ì˜ ì¶œë ¥ì„ ê·¸ëŒ€ë¡œ ë°€ì§‘ì¸µì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

**09-1 í•µì‹¬ í‚¤ì›Œë“œ**

- **ìˆœì°¨ ë°ì´í„°**: í…ìŠ¤íŠ¸ë‚˜ ì‹œê³„ì—´ ë°ì´í„°ì™€ ê°™ì´ ìˆœì„œì— ì˜ë¯¸ê°€ ìˆëŠ” ë°ì´í„°
- **ìˆœí™˜ ì‹ ê²½ë§**: ìˆœì°¨ ë°ì´í„°ì— ì˜ ë§ëŠ” ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ ì¢…ë¥˜.
- ì…€: ìˆœí™˜ ì‹ ê²½ë§ì—ì„œ ìˆœí™˜ì¸µì„ ì¼ì»«ëŠ” ë§.
- **ì€ë‹‰ ìƒíƒœ**: ìˆœí™˜ ì‹ ê²½ë§ì—ì„œ ì…€ì˜ ì¶œë ¥ì„ ì¼ì»«ëŠ” ë§.

---

<br>

### **09-2 RNNìœ¼ë¡œ IMDB ë¦¬ë·° ë¶„ë¥˜í•˜ê¸°**

IMDB ë¦¬ë·° ë°ì´í„°ì…‹ì€ ìœ ëª…í•œ ì¸í„°ë„· ì˜í™” ë°ì´í„°ë² ì´ìŠ¤ì¸ imdb.comì—ì„œ ìˆ˜ì§‘í•œ ë¦¬ë·°ë¥¼ ê°ìƒí‰ì— ë”°ë¼ ê¸ì •ê³¼ ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•´ ë†“ì€ ë°ì´í„°ì…‹ì´ë‹¤.

í…ìŠ¤íŠ¸ ìì²´ë¥¼ ì‹ ê²½ë§ì— ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë‹¨ì–´ë§ˆë‹¤ ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•œë‹¤. ì´ ë•Œ 0ì€ íŒ¨ë”©, 1ì€ ë¬¸ì¥ì˜ ì‹œì‘, 2ëŠ” ì–´íœ˜ ì‚¬ì „ì— ì—†ëŠ” í† í°ìœ¼ë¡œ ë¯¸ë¦¬ ë¶€ì—¬ê°€ ë˜ì–´ìˆë‹¤. ì´ë ‡ê²Œ ë¶„ë¦¬ëœ ë‹¨ì–´ë¥¼ **í† í°(token)**ì´ë¼ê³  ë¶€ë¥¸ë‹¤. í•˜ë‚˜ì˜ ìƒ˜í”Œì€ ì—¬ëŸ¬ ê°œì˜ í† í°ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , **1ê°œì˜ í† í°ì´ í•˜ë‚˜ì˜ íƒ€ì„ìŠ¤í…ì— í•´ë‹¹**í•œë‹¤. 

```python
from tensorflow.keras.datasets import imdb
(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)
#num_wordsë§¤ê°œë³€ìˆ˜ë¥¼ 500ìœ¼ë¡œ í•˜ì—¬ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ 500ê°œë§Œ ì‚¬ìš©

print(len(train_input[0])) #ì²«ë²ˆì§¸ ë¦¬ë·°ì˜ ê¸¸ì´
#ê²°ê³¼ê°’
218

print(train_target[:20]) #íƒ€ê¹ƒ 20ê°œ ì¶œë ¥
#ê²°ê³¼ê°’
[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1] #0ì€ ë¶€ì •, 1ì€ ê¸ì •

```

```python
#ê²€ì¦ì„¸íŠ¸ ë¶„ë¦¬
from sklearn.model_selection import train_test_split
train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)

import numpy as np
lengths = np.array([len(x) for x in train_input]) #ê° ë¦¬ë·°ì˜ ê¸¸ì´ë¥¼ ë°°ì—´ë¡œ ë§Œë“¤ê¸°
print(np.mean(lengths), np.median(lengths)) #ê¸¸ì´ì˜ í‰ê· ê³¼ ì¤‘ê°„ê°’
#ê²°ê³¼ê°’
239.00925 178.0 #í‰ê·  ë‹¨ì–´ ìˆ˜ëŠ” 239ê°œ, ì¤‘ê°„ê°’ì€ 178ê°œ.
```

í‰ê· ê°’ì´ 239, ì¤‘ê°„ê°’ì´ 178ì¸ ê²ƒìœ¼ë¡œ ë³´ì•„ ì´ ë¦¬ë·° ê¸¸ì´ ë°ì´í„°ëŠ” í•œìª½ì— ì¹˜ìš°ì³¤ì„ ê²ƒì´ë‹¤. íˆìŠ¤í† ê·¸ë¨ì„ í™•ì¸í•´ë³´ì.

<img src="https://user-images.githubusercontent.com/115082062/206373814-12e2af61-b95c-427d-b49d-123500634ded.jpg">

ëŒ€ë¶€ë¶„ì´ 300 ë¯¸ë§Œì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ë¦¬ë·°ì˜ ê¸¸ì´ê°€ 100 ë¯¸ë§Œì¸ ê²ƒë§Œ ì‚¬ìš©í•´ë„ ì¶©ë¶„í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ê¸¸ì´ê°€ 100ì„ ë„˜ìœ¼ë©´ 100ì— ë§ì¶”ì–´ ì˜ë¼ë‚´ê³ , 100ë³´ë‹¤ ì§§ìœ¼ë©´ í† í° 0ìœ¼ë¡œ íŒ¨ë”©ì„ í•´ì¤€ë‹¤. ì¼€ë¼ìŠ¤ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶”ëŠ” `pad_sequences()`í•¨ìˆ˜ë¥¼ ì œê³µí•œë‹¤.

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
train_seq = pad_sequences(train_input, maxlen=100) #maxlenë§¤ê°œë³€ìˆ˜ì— ì›í•˜ëŠ” ê¸¸ì´ë¥¼ ì§€ì •
val_seq = pad_sequences(val_input, maxlen=100) #ê²€ì¦ì„¸íŠ¸ì˜ ê¸¸ì´ë„ 100ìœ¼ë¡œ

print(train_seq.shape)
#ê²°ê³¼ê°’
(20000, 100)  #í† í° 100ê°œì˜ ìƒ˜í”Œì´ 20,000ê°œ ìˆë‹¤.
```

ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ì— ìˆëŠ” ë‹¨ì–´ê°€ ì…€ì˜ ì€ë‹‰ìƒíƒœì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ ë§ˆì§€ë§‰ì— íŒ¨ë”©ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì€ ì„ í˜¸ë˜ì§€ ì•ŠëŠ”ë‹¤. ë•Œë¬¸ì— ìë™ìœ¼ë¡œ íŒ¨ë”©ì€ ì•ì— ìœ„ì¹˜í•œë‹¤.

ì´ì œ ìˆœí™˜ ì‹ ê²½ë§ì„ ë§Œë“¤ì–´ë³´ì. ì¼€ë¼ìŠ¤ëŠ” `simpleRNN` í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤.

```python
from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.SimpleRNN(8,input_shape=(100,500))) 
#Denseë‚˜ Conv2D í´ë˜ìŠ¤ ëŒ€ì‹  SimpleRNN ì‚¬ìš©
#activationì˜ ê¸°ë³¸ê°’ì€ tanhì´ë‹¤.
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 simple_rnn (SimpleRNN)      (None, 8)                 4072      
                                                                 
 dense (Dense)               (None, 1)                 9         
                                                                 
=================================================================
Total params: 4,081
Trainable params: 4,081
Non-trainable params: 0
_________________________________________________________________
```

ë˜í•œ í† í°ì— ë¶€ì—¬ëœ ì •ìˆ˜ê°€ í¬ê¸°ì˜ ì˜ë¯¸ë¥¼ ì§€ë…€ì„  ì•ˆ ëœë‹¤. 100ì´ ë¶€ì—¬ëœ ë‹¨ì–´ê°€ 5ê°€ ë¶€ì—¬ëœ ë‹¨ì–´ë³´ë‹¤ 20ë°° ì¤‘ìš”í•œ ê²ƒì´ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ ì •ìˆ«ê°’ì— ìˆëŠ” í¬ê¸° ì†ì„±ì„ ì—†ì• ê³  ê° ì •ìˆ˜ë¥¼ ê³ ìœ í•˜ê²Œ í‘œí˜„í•˜ê¸° ìœ„í•´ **ì›-í•«ì¸ì½”ë”©**ì„ í•´ì¤€ë‹¤.

```python
#ì›í•«ì¸ì½”ë”©
train_oh = keras.utils.to_categorical(train_seq)
val_oh = keras.utils.to_categorical(val_seq)
```

```python
#ìˆœí™˜ ì‹ ê²½ë§ í›ˆë ¨
rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5',save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model.fit(train_oh, train_target, epochs=100, batch_size=64, validation_data=(val_oh, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

#ê²°ê³¼
Epoch 36/100 #36ë²ˆì§¸ ì—í¬í¬ì—ì„œ ì¤‘ë‹¨
313/313  15s 47ms/step - loss: 0.4060 - accuracy: 0.8189 - val_loss: 0.4488 - val_accuracy: 0.7944

```

í•˜ì§€ë§Œ ì´ ì‘ì—…ì—ì„œ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í•˜ë©´ ì…ë ¥ ë°ì´í„°ê°€ ë§¤ìš° ì»¤ì§„ë‹¤. í† í° 1ê°œê°€ 500ì°¨ì›ìœ¼ë¡œ ëŠ˜ì–´ë‚¬ê¸° ë•Œë¬¸ì´ë‹¤.

ë•Œë¬¸ì— ì›-í•«ì¸ì½”ë”©ë³´ë‹¤ë„ **ë‹¨ì–´ ì„ë² ë”©(word embedding)**ì„ ì¦ê²¨ ì‚¬ìš©í•œë‹¤. ë‹¨ì–´ ì„ë² ë”©ì€ ê° ë‹¨ì–´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë°”ê¿” ì¤€ë‹¤. ì´ëŠ” ì›-í•« ì¸ì½”ë”©ë³´ë‹¤ í›¨ì”¬ ì˜ë¯¸ ìˆëŠ” ê°’ìœ¼ë¡œ ì±„ì›Œì ¸ ìˆê¸°ì— ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤.

```python
model2=keras.Sequential()
model2.add(keras.layers.Embedding(500, 16, input_length=100))
#ì²«ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ëŠ” ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸°
#ë‘ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ëŠ” ì„ë² ë”© ë²¡í„°ì˜ í¬ê¸°. ì›-í•«ì¸ì½”ë”©ì€ 500ê°œì¸ë°, ì—¬ê¸°ì„  16ê°œë°–ì— ì•ˆ ëœë‹¤
#ì„¸ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´.
model2.add(keras.layers.SimpleRNN(8))
model2.add(keras.layers.Dense(1, activation='sigmoid'))

model2.summary()
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 100, 16)           8000      
                                                                 
 simple_rnn_1 (SimpleRNN)    (None, 8)                 200       
                                                                 
 dense_1 (Dense)             (None, 1)                 9         
                                                                 
=================================================================
Total params: 8,209
Trainable params: 8,209
Non-trainable params: 0
_________________________________________________________________
```

```python
#ë‹¨ì–´ì„ë² ë”© ìˆœí™˜ì‹ ê²½ë§ í›ˆë ¨
rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5',save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

#ê²°ê³¼
Epoch 40/100
313/313 - 8s 24ms/step - loss: 0.3829 - accuracy: 0.8396 - val_loss: 0.4562 - val_accuracy: 0.7870
```

**09-2 í•µì‹¬ í‚¤ì›Œë“œ**

- **í† í°**: í…ìŠ¤íŠ¸ì—ì„œ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ë˜ëŠ” ë¬¸ìì—´
- **ì›-í•« ì¸ì½”ë”©**: ì–´ë–¤ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì›ì†Œë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ì¸ ë²¡í„°
- **ë‹¨ì–´ ì„ë² ë”©**: ì •ìˆ˜ë¡œ ë³€í™˜ëœ í† í°ì„ ë¹„êµì  ì‘ì€ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë°€ì§‘ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•

**09-2 í•µì‹¬ íŒ¨í‚¤ì§€ì™€ í•¨ìˆ˜**

- TensorFlow
    - **`pad_sequences()`**: ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ íŒ¨ë”©ì„ ì¶”ê°€
        - **`maxlen`**ë§¤ê°œë³€ìˆ˜ë¡œ ì›í•˜ëŠ” ì‹œí€€ìŠ¤ ê¸¸ì´ ì§€ì •
        - `padding`ë§¤ê°œë³€ìˆ˜ëŠ” íŒ¨ë”©ì„ ì¶”ê°€í•  ìœ„ì¹˜ë¥¼ ì§€ì •. ê¸°ë³¸ê°’ì¸ â€˜preâ€™ëŠ” ì‹œí€€ìŠ¤ ì•ì— íŒ¨ë”©ì„ ì¶”ê°€
    - `to_categorical()`: ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜
    - **`SimpleRNN`**: ì¼€ë¼ìŠ¤ì˜ ê¸°ë³¸ ìˆœí™˜ì¸µ í´ë˜ìŠ¤
        - ì²«ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ì— ë‰´ëŸ°ì˜ ê°œìˆ˜ ì§€ì •
        - `activation`ë§¤ê°œë³€ìˆ˜ì— í™œì„±í™”í•¨ìˆ˜ ì§€ì •. ê¸°ë³¸ê°’ì€ **tanh**
        - `dropout`ë§¤ê°œë³€ìˆ˜ì— ì…ë ¥ì— ëŒ€í•œ ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì§€ì •
    - **`Embedding`**: ë‹¨ì–´ ì„ë² ë”©ì„ ìœ„í•œ í´ë˜ìŠ¤
        - ì²«ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ì— ì–´íœ˜ ì‚¬ì „ì˜ í¬ê¸° ì§€ì •
        - ë‘ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ì— ì¶œë ¥í•  ë°€ì§‘ ë²¡í„°ì˜ í¬ê¸° ì§€ì •
        - `input_length`ë§¤ê°œë³€ìˆ˜ì— ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ ì§€ì •
        

---

<br>

### 09-3 LSTM

LSTMì€ Long Short-Term Memoryì˜ ì•½ìë¡œ, ë‹¨ê¸° ê¸°ì–µì„ ì˜¤ë˜ ê¸°ì–µí•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ ì…€ì´ë‹¤.

LSTMì—ëŠ” ì€ë‹‰ìƒíƒœ ë§ê³ ë„ ì…€ ìƒíƒœë¼ê³  ë¶€ë¥´ëŠ” ê°’ì´ ìˆë‹¤. ì´ ì…€ ìƒíƒœëŠ” ë‹¤ìŒ ì¸µìœ¼ë¡œ ì „ë‹¬ë˜ì§€ ì•Šê³  LSTM ì…€ì—ì„œ ìˆœí™˜ë§Œ ë˜ëŠ” ê°’ì´ë‹¤.

<img src="https://user-images.githubusercontent.com/115082062/206374327-36a0e3da-d444-4b8d-97fb-af15f9ce39bd.png">

 LSTMì˜ ëŒ€ëµì  êµ¬ì¡°

ì‚­ì œ ê²Œì´íŠ¸ëŠ” ì…€ ìƒíƒœì— ìˆëŠ” ì •ë³´ë¥¼ ì œê±°í•˜ëŠ” ì—­í• ì„ í•˜ê³  ì…ë ¥ ê²Œì´íŠ¸ëŠ” ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì…€ ìƒíƒœì— ì¶”ê°€í•˜ë©° ì¶œë ¥ ê²Œì´íŠ¸ë¥¼ í†µí•´ì„œ ì´ ì…€ ìƒíƒœê°€ ë‹¤ìŒ ì€ë‹‰ ìƒíƒœë¡œ ì¶œë ¥ëœë‹¤.

ì´ì œ LSTM ì‹ ê²½ë§ì„ í›ˆë ¨í•´ë³´ì

```python
from tensorflow.keras.datasets import imdb
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)
train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)

from tensorflow.keras.preprocessing.sequence import pad_sequences
#ê¸¸ì´ë¥¼ 100ìœ¼ë¡œ ë§ì¶”ì–´ íŒ¨ë”©
train_seq = pad_sequences(train_input, maxlen=100)
val_seq = pad_sequences(val_input, maxlen=100)
```

ìˆœí™˜ì¸µì—ì„œ `SimpleRNN` í´ë˜ìŠ¤ë¥¼ `LSTM` í´ë˜ìŠ¤ë¡œ ë°”ê¿”ì£¼ê¸°ë§Œ í•˜ë©´ ëœë‹¤.

```python
#ìˆœí™˜ì¸µ ë§Œë“¤ê¸° 
from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.Embedding(500,16,input_length=100))
model.add(keras.layers.LSTM(8))
model.add(keras.layers.Dense(1,activation='sigmoid'))
```

```python
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 100, 16)           8000      
                                                                 
 lstm (LSTM)                 (None, 8)                 800       
                                                                 
 dense (Dense)               (None, 1)                 9         
                                                                 
=================================================================
Total params: 8,809
Trainable params: 8,809
Non-trainable params: 0
_________________________________________________________________
```

`SimpleRNN` í´ë˜ìŠ¤ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê°œìˆ˜ëŠ” 200ê°œì˜€ë‹¤. `LSTM` ì…€ì—ëŠ” ì‘ì€ ì…€ì´ 4ê°œ ìˆìœ¼ë¯€ë¡œ ì •í™•íˆ 4ë°° ëŠ˜ì–´ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ 800ê°œì´ë‹¤.

ì´ì œ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³  í›ˆë ¨í•œ ë‹¤ìŒ í›ˆë ¨ì†ì‹¤ê³¼ ê²€ì¦ ì†ì‹¤ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë³´ì.

```python
rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer = rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5', save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

#ê²°ê³¼
Epoch 27/100
313/313 - 12s 38ms/step - loss: 0.4164 - accuracy: 0.8139 - val_loss: 0.4396 - val_accuracy: 0.8024
```

```python
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()
```

<img src="https://user-images.githubusercontent.com/115082062/206374424-2bf592ba-2dd5-47e0-a166-e474af292523.jpg">

ê¸°ë³¸ ìˆœí™˜ì¸µë³´ë‹¤ LSTMì´ ê³¼ëŒ€ì í•©ì„ ì˜ ì–µì œí•˜ë©´ì„œ í›ˆë ¨ì„ ì˜ ìˆ˜í–‰í•œ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
---
title: "[NLP] BERT í•™ìŠµ ë°©ì‹ MLMê³¼ NSP ì—°ìŠµ"
excerpt: "BERTì˜ pretrain ë°©ì‹ì¸ MLM(Masked LM)ê³¼ NSP(Nest Sentence Prediction)ì„ êµ¬í˜„í•´ë³´ì"
toc: true
toc_label: "ëª©ì°¨"
toc_sticky: true

tags: [Deep Learning, NLP, BERT, MLM, NSP]

published: true

categories:
  - DL

date: 2023-02-28 21:00:00
last_modified_at: 2023-02-28 21:00:00
---


<br>

<div class="notice--primary" markdown="1">
ğŸ’¡ êµë‚´ í•™íšŒ NLP ë¶„ë°˜ì—ì„œ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•œ í¬ìŠ¤íŒ…ì…ë‹ˆë‹¤.
</div>

<br>

## 1. ê°œìš”
BERTëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ë¡œ pretrainì‹œí‚¤ê³ , ì´í›„ì— ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ(fine tuning)ì„ í•˜ì—¬ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ëª¨ë¸ì´ë‹¤.

pretrainì˜ ì¢…ë¥˜ëŠ” í¬ê²Œ ë‘ ê°€ì§€ê°€ ìˆë‹¤.

1) **Mask LM**: ì…ë ¥ ë‹¨ì–´ì˜ 15%ë¥¼ maskingí•˜ê³  ì´ë¥¼ ì˜ˆì¸¡í•˜ê²Œ í•¨.

  EX) ë‚˜ëŠ” ìš´ì¢‹ê²Œë„ ìˆ˜ê°•ì‹ ì²­ì—ì„œ [MASK] ì„ í•  ìˆ˜ ìˆì—ˆë‹¤. â‡’ [ì˜¬í´]

  ì´ ë•Œ 15%ë¥¼ ëª¨ë‘ MASK ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤.

  - 12%: [MASK]ë¡œ ë³€ê²½
  - 1.5%: ì„ì˜ ë‹¨ì–´ë¡œ ëœë¤ ë³€ê²½
  - 1.5%: ë³€ê²½ X

2) **NSP**: ë‘ ê°œì˜ ë¬¸ì¥ì´ ì£¼ì–´ì§€ë©´, ì´ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ë§ì¶”ëŠ” ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ì‹œí‚´.(50%ëŠ” ì´ì–´ì§€ëŠ” ë¬¸ì¥, 50%ëŠ” ëœë¤ìœ¼ë¡œ ì„ íƒëœ ì•ˆ ì´ì–´ì§€ëŠ” ë¬¸ì¥)


pretrainì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì…‹ì€ ìœ„í‚¤í”¼ë””ì•„(25ì–µ ë‹¨ì–´)ì™€ Books Corpus(8ì–µ ë‹¨ì–´)ì´ë‹¤.

<br>

## 2. MLM(Masked LM)


```python
!pip install transformers
```

    Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
    Collecting transformers
      Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m6.3/6.3 MB[0m [31m19.9 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)
    Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)
    Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)
    Collecting huggingface-hub<1.0,>=0.11.0
      Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m190.3/190.3 KB[0m [31m11.3 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)
    Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)
    Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)
    Collecting tokenizers!=0.11.3,<0.14,>=0.11.1
      Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m7.6/7.6 MB[0m [31m52.5 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)
    Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)
    Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)
    Installing collected packages: tokenizers, huggingface-hub, transformers
    Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.1
    


```python
from transformers import BertTokenizer, BertForMaskedLM
import torch
```


```python
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

text = ("After Abraham Lincoln won the November 1860 presidential "
        "election on an anti-slavery platform, an initial seven "
        "slave states declared their secession from the country "
        "to form the Confederacy. War broke out in April 1861 "
        "when secessionist forces attacked Fort Sumter in South "
        "Carolina, just over a month after Lincoln's "
        "inauguration.")
```


    Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]



    Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]



    Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]



    Downloading (â€¦)"pytorch_model.bin";:   0%|          | 0.00/440M [00:00<?, ?B/s]


    Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
    - This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    


```python
inputs = tokenizer(text, return_tensors='pt') # return_tensorsë¥¼ 'pt'ë¡œ ì„¤ì •í•˜ë©´ pytorch tesnor ìë£Œí˜•ìœ¼ë¡œ ë°˜í™˜í•´ì¤€ë‹¤.
```

í† í¬ë‚˜ì´ì €ë¥¼ í†µê³¼ì‹œí‚¤ë©´ `input_ids`, `token_type_ids`, `attention_mask` ì„¸ ê°œì˜ ì„ë² ë”©ì„ ë°˜í™˜í•´ì¤€ë‹¤.

- `input_ids`: ê° ë‹¨ì–´ë¥¼ ì •ìˆ˜ ì¸ì½”ë”©í•´ì¤€ ê²ƒ
- `token_type_ids`: í•´ë‹¹ ë‹¨ì–´ê°€ ì²«ë²ˆì§¸ ë¬¸ì¥(0)ì¸ì§€ ë‘ë²ˆì§¸ ë¬¸ì¥(1)ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ì£¼ëŠ” ê²ƒ.
- `attention_mask`: attention layerê°€ ë¬´ì‹œí•´ì•¼ í•˜ëŠ” íŒ¨ë”© í† í°ë“¤ì€ 0ìœ¼ë¡œ ì§€ì •, ì•„ë‹Œ ê²ƒì€ 1ë¡œ ì§€ì •



```python
inputs.keys()
```




    dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])




```python
inputs['input_ids'] #101ì€ bos í† í°, 102ëŠ” eos í† í°ì´ë‹¤.
```




    tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,  2602,
              2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,
              6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,
              1996, 18179,  1012,  2162,  3631,  2041,  1999,  2258,  6863,  2043,
             22965,  2923,  2749,  4457,  3481,  7680,  3334,  1999,  2148,  3792,
              1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055, 17331,
              1012,   102]])




```python
# í•˜ë‚˜ì˜ ë¬¸ì¥ë§Œ ë“¤ì–´ê°”ê¸° ë•Œë¬¸ì— ëª¨ë“  ë‹¨ì–´ê°€ 0(ì²«ë²ˆì§¸ ë¬¸ì¥)ìœ¼ë¡œ í‘œì‹œëœë‹¤.
inputs['token_type_ids']
```




    tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])




```python
# íŒ¨ë”© í† í°ì´ ì—†ê¸°ì—, ëª¨ë“  ë‹¨ì–´ê°€ attention layerì˜ ì ìš©ì„ ë°›ë„ë¡ 1ë¡œ í‘œì‹œëœë‹¤.
inputs['attention_mask']
```




    tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])




```python
inputs['labels'] = inputs.input_ids.detach().clone()
```

ì´ì œ íŠ¹ì • ë‹¨ì–´ë“¤ì— Maskingì„ í•´ì¤„ ê²ƒì´ë‹¤. MaskëŠ” ì „ì²´ ë‹¨ì–´ ì¤‘ 15%ì— í•œí•˜ì—¬ ì§„í–‰í•œë‹¤. ê·¸ë˜ì„œ ë‹¨ì–´ë“¤ì„ ëœë¤ìœ¼ë¡œ 0ê³¼ 1ì‚¬ì´ floatìœ¼ë¡œ ìƒì„±í•˜ê³ , ì´ ì¤‘ 15%ë§Œ Maskingì„ í•´ì¤„ ê²ƒì´ë‹¤.


```python
# input_idsì™€ ê°™ì€ ì°¨ì›ì˜ ëœë¤ float arrayë¥¼ ë§Œë“ ë‹¤.
rand = torch.rand(inputs.input_ids.shape)
# ì´ ë•Œ 0.15ë³´ë‹¤ ì‘ì€ ìš”ì†Œë“¤ì„ì„ Trueë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
mask_arr = rand < 0.15
mask_arr
```




    tensor([[False, False, False,  True, False, False, False, False, False, False,
             False, False, False, False, False, False, False, False, False, False,
              True, False, False, False, False, False, False, False, False, False,
             False, False, False,  True,  True, False,  True, False, False, False,
              True, False, False,  True, False, False, False, False, False, False,
             False, False, False, False, False, False, False, False, False, False,
             False,  True]])




```python
# bos, eosë„ Falseë¡œ ë§Œë“¤ê¸° ìœ„í•´ ì•„ë˜ì²˜ëŸ¼ ì¡°ê±´ì„ ë” ê±´ë‹¤.
mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102)
mask_arr
```




    tensor([[False, False, False,  True, False, False, False, False, False, False,
             False, False, False, False, False, False, False, False, False, False,
              True, False, False, False, False, False, False, False, False, False,
             False, False, False,  True,  True, False,  True, False, False, False,
              True, False, False,  True, False, False, False, False, False, False,
             False, False, False, False, False, False, False, False, False, False,
             False, False]])




```python
# Trueì¸ ê°’ë“¤ì˜ indexë¥¼ flattení•˜ì—¬ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ì— ë‹´ëŠ”ë‹¤
selection = torch.flatten((mask_arr[0]).nonzero()).tolist()
selection
```




    [3, 20, 33, 34, 36, 40, 43]




```python
# Mask í† í°ì€ 103ìœ¼ë¡œ ì§€ì •í•œë‹¤. ì•ì„œ ì–»ì€ indexë“¤ì„ 103ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤.
inputs.input_ids[0, selection] = 103
```


```python
inputs['input_ids'] # 103ìœ¼ë¡œ ë³€í•œ ë¶€ë¶„ì´ Mask ì²˜ë¦¬ëœ ë¶€ë¶„ì´ë‹¤.
```




    tensor([[  101,  2044,  8181,   103,  2180,  1996,  2281,  7313,  4883,  2602,
              2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,
               103,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,
              1996, 18179,  1012,   103,   103,  2041,   103,  2258,  6863,  2043,
               103,  2923,  2749,   103,  3481,  7680,  3334,  1999,  2148,  3792,
              1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055, 17331,
              1012,   102]])



ì´ì œ ë§Œë“¤ì–´ì§„ inputsì„ BERT ëª¨ë¸ì— ë„£ëŠ”ë‹¤. loss ê°’ì„ ë°˜í™˜í•´ì£¼ëŠ”ë°, ì´ëŠ” Maskëœ ë¶€ë¶„ì˜ ë‹¨ì–´ì— í•œí•œ lossì´ë‹¤.


```python
outputs = model(**inputs)
```


```python
outputs.keys()
```




    odict_keys(['loss', 'logits'])




```python
outputs.loss
```




    tensor(0.8053, grad_fn=<NllLossBackward0>)




```python
outputs.logits
```




    tensor([[[ -7.1162,  -7.0597,  -7.0891,  ...,  -6.3137,  -6.1457,  -4.3020],
             [ -9.2592,  -9.0717,  -9.2359,  ...,  -8.8328,  -8.2930,  -7.9929],
             [ -8.1557,  -8.7103,  -8.2165,  ...,  -7.6762,  -6.8691,  -7.1768],
             ...,
             [ -1.4756,  -1.3714,  -1.3981,  ...,  -0.9416,  -0.5452,  -7.5062],
             [-13.9242, -13.8415, -13.8469,  ..., -10.8817, -11.1650,  -9.2146],
             [-11.8421, -12.2480, -11.7827,  ..., -11.7742,  -9.1722,  -9.1381]]],
           grad_fn=<ViewBackward0>)


<br>

## 3. NSP(Next Sentence Prediction)
ë°©ì‹ì€ MLMê³¼ ê±°ì˜ ìœ ì‚¬í•˜ë‹¤. ë‹¤ë§Œ ë‘ ê°œì˜ ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ë„£ëŠ”ë‹¤.


```python
from transformers import BertTokenizer, BertForNextSentencePrediction

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')

text = ("After Abraham Lincoln won the November 1860 presidential election on an "
        "anti-slavery platform, an initial seven slave states declared their "
        "secession from the country to form the Confederacy.")
text2 = ("War broke out in April 1861 when secessionist forces attacked Fort "
         "Sumter in South Carolina, just over a month after Lincoln's "
         "inauguration.")
```

    Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
    - This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    


```python
inputs = tokenizer(text, text2, return_tensors='pt')
inputs.keys()
```




    dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])




```python
inputs['token_type_ids'] # ë¬¸ì¥ì„ ë‘ ê°œë¥¼ ë„£ì—ˆìœ¼ë‹ˆ 0(ì²«ë²ˆì§¸ ë¬¸ì¥)ê³¼ 1(ë‘ë²ˆì§¸ ë¬¸ì¥)ë¡œ êµ¬ë¶„ì´ ëœë‹¤.
```




    tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])



labelì€ 0(is next sentence)ê³¼ 1(not next sentence)ë¡œ ë‚˜ë‰˜ê²Œ ëœë‹¤.


```python
labels = torch.LongTensor([0])
```


```python
outputs = model(**inputs, labels=labels)
outputs.keys()
```




    odict_keys(['loss', 'logits'])




```python
outputs.loss
```




    tensor(3.2186e-06, grad_fn=<NllLossBackward0>)



`logits`ì—ì„œ ë°˜í™˜ë˜ëŠ” ê°’ì„ ë³´ë©´ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì´ëƒ, ì•„ë‹ˆëƒë¼ëŠ” ì´ì§„ ë¶„ë¥˜ì´ê¸° ë•Œë¬¸ì— ê°’ì´ ë‘ ê°œë§Œ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

ì•ì„œ inputìœ¼ë¡œ ë„£ì€ ë‘ ë¬¸ì¥ì€ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì´ì—ˆê¸° ë•Œë¬¸ì— `argmax` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ 0ì´ ë°˜í™˜ëœë‹¤.


```python
outputs.logits
```




    tensor([[ 6.3968, -6.2638]], grad_fn=<AddmmBackward0>)




```python
torch.argmax(outputs.logits)
```




    tensor(0)



ì´ë²ˆì—” ì˜ ì—°ê´€ì´ ì—†ëŠ” ë¬¸ì¥ì„ text2ë¡œ ë„£ì–´ë³´ì.


```python
text = ("After Abraham Lincoln won the November 1860 presidential election on an "
        "anti-slavery platform, an initial seven slave states declared their "
        "secession from the country to form the Confederacy.")
text2 = ("Pele began playing for Santos at age 15 and the Brazil national team "
         "at 16. During his international career, he won three FIFA World Cups: 1958, 1962 and 1970, "
         "the only player to do so.")
```


```python
inputs = tokenizer(text, text2, return_tensors='pt')
```


```python
outputs = model(**inputs, labels=labels)
```


```python
outputs.loss
```




    tensor(12.9636, grad_fn=<NllLossBackward0>)




```python
outputs.logits
```




    tensor([[-5.0046,  7.9590]], grad_fn=<AddmmBackward0>)



ê·¸ëŸ¼ 1ì´ ë°˜í™˜ë˜ë©° ë‘ ë¬¸ì¥ì´ ì´ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì˜ ë¶„ë¥˜í•´ë‚¸ë‹¤.


```python
torch.argmax(outputs.logits)
```




    tensor(1)



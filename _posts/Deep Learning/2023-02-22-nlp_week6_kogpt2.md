---
title: "[NLP] KoGPT2ë¡œ ì´ì–´ì§ˆ ë¬¸ì¥ì„ ì˜ˆì¸¡í•´ë³´ì"
excerpt: "ë¬¸ì¥ ìƒì„±ì— íŠ¹í™”ëœ KoGPT2ë¡œ í•œêµ­ì–´ ë¬¸ì¥ì„ ìƒì„±í•´ë³´ì"
toc: true
toc_label: "ëª©ì°¨"
toc_sticky: true

tags: [Deep Learning, NLP, KoGPT2, pytorch, keras]

published: true

categories:
  - DL

date: 2023-02-22 21:00:00
last_modified_at: 2023-02-22 21:00:00
---


<br>

<div class="notice--primary" markdown="1">
ğŸ’¡ êµë‚´ í•™íšŒ NLP ë¶„ë°˜ì—ì„œ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•œ í¬ìŠ¤íŒ…ì…ë‹ˆë‹¤.
</div>

<br>

GPT-2ëŠ” ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµëœ ì–¸ì–´ëª¨ë¸ì´ë©° ë¬¸ì¥ ìƒì„±ì— ìµœì í™” ë˜ì–´ ìˆë‹¤. KoGPT-2ëŠ” ì´ë¦„ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯, í•œêµ­ì–´ì˜ ì–¸ì–´ì  í•œê³„ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•œêµ­ì–´ ì „ìš© ì–¸ì–´ëª¨ë¸ì´ë‹¤.

BERTê°€ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ë””ì½”ë”ë¥¼ ì§€ìš°ê³  ì¸ì½”ë”ë§Œ ë‚¨ê²¼ë˜ ê²ƒê³¼ ì™„ì „íˆ ë°˜ëŒ€ë¡œ, GPT-2ëŠ” ë””ì½”ë”ë§Œ ë‚¨ê¸´ë‹¤. ë˜í•œ BERTì™€ ë‹¬ë¦¬ GPT-2ëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì„ ì¶œë ¥í•˜ê³  ì¶œë ¥ í† í°ì„ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì¦‰, ê³¼ê±° ì–¸ì–´ëª¨ë¸ê³¼ëŠ” ìœ ì‚¬í•œ êµ¬ì¡°ë¥¼ ë¤ë‹¤.

<br>

## KoGPT2ë¡œ ì´ì–´ì§€ëŠ” ë¬¸ì¥ ìƒì„±


```python
!pip install transformers
```

    Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
    Collecting transformers
      Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m6.3/6.3 MB[0m [31m38.3 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)
    Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)
    Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)
    Collecting huggingface-hub<1.0,>=0.11.0
      Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m190.3/190.3 KB[0m [31m10.9 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)
    Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)
    Collecting tokenizers!=0.11.3,<0.14,>=0.11.1
      Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m7.6/7.6 MB[0m [31m56.8 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)
    Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)
    Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)
    Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)
    Installing collected packages: tokenizers, huggingface-hub, transformers
    Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.1
    

huggingfaceì— ë°°í¬ëœ ëª¨ë¸ë“¤ì€ Pytorchë¡œ í•™ìŠµëœ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì´ë‹¤. ë§Œì•½ Tensorflowì—ì„œ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ from_pretrained() ì¸ìì— `from_pt=True`ë¥¼ ë„£ì–´ì¤Œìœ¼ë¡œì¨ Tensorflowëª¨ë¸ë¡œ ë¡œë“œí•  ìˆ˜ ìˆë‹¤.

pytorchì™€ tensorflowë¡œ ëª¨ë‘ ì§„í–‰í•´ë³´ê² ë‹¤.


```python
import torch
from transformers import PreTrainedTokenizerFast
from transformers import GPT2LMHeadModel

tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2')
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
```

    The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
    The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. 
    The class this function is called from is 'PreTrainedTokenizerFast'.
    


```python
text = 'ìˆ˜ê°•ì‹ ì²­ì—ì„œ ì›í•˜ëŠ” ê°•ì˜ë¥¼ ì‹ ì²­í•˜ê¸° ìœ„í•´ì„œëŠ”'

input_ids = tokenizer.encode(text, return_tensors='pt')
summary_ids = model.generate(input_ids,
                           max_length=128,
                           repetition_penalty=2.0,
                           use_cache=True)
```


```python
decoding_ids = tokenizer.decode(summary_ids[0])
print(decoding_ids)
```

    ìˆ˜ê°•ì‹ ì²­ì—ì„œ ì›í•˜ëŠ” ê°•ì˜ë¥¼ ì‹ ì²­í•˜ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ í•´ë‹¹ ê°•ì˜ì— ëŒ€í•œ ì‚¬ì „ì •ë³´ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤.
    ë˜í•œ ê° ê°•ì¢Œë³„ ì „ë¬¸ê°•ì‚¬ì§„ì˜ ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì§ˆì˜ì‘ë‹µ ì‹œê°„ë„ ë§ˆë ¨ë¼ ìˆë‹¤.
    ì´ë²ˆ ê³¼ì •ì€ ì˜¤ëŠ” 9ì›” 30ì¼ê¹Œì§€ ë§¤ì£¼ í™”ìš”ì¼ ì˜¤í›„ 2ì‹œë¶€í„° 4ì‹œê¹Œì§€ ì´ 6íšŒì— ê±¸ì³ ì§„í–‰ëœë‹¤.
    ìˆ˜ê°•ì„ í¬ë§í•˜ëŠ” ì‚¬ëŒì€ ëˆ„êµ¬ë‚˜ ë¬´ë£Œë¡œ ì°¸ì—¬í•  ìˆ˜ ìˆìœ¼ë©°, ìì„¸í•œ ì‚¬í•­ì€ í™ˆí˜ì´ì§€(www.suwonline.kr)ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ë„ ìˆë‹¤.</d> ì§€ë‚œí•´ 12ì›” 31ì¼ë¶€í„° ì˜¬ 1ì›” 1ì¼ ì‚¬ì´ì— ë°œìƒí•œ êµí†µì‚¬ê³  ì‚¬ë§ì ìˆ˜ëŠ” ëª¨ë‘ 1ë§Œ2ì²œ9ë°±ì—¬ ëª…ìœ¼ë¡œ ì§‘ê³„ëìŠµë‹ˆë‹¤.
    ì´ëŠ” ì „ë…„ ê°™ì€ ê¸°ê°„ë³´ë‹¤ 8% ì¦ê°€í•œ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.
    íŠ¹íˆ, ì „ì²´ ì‚¬ê³ 
    


```python
print(input_ids)
print(summary_ids)
```

    tensor([[32619, 10309,  9023, 15605, 25175, 48565, 11357]])
    tensor([[32619, 10309,  9023, 15605, 25175, 48565, 11357, 11488,  9992, 11631,
              8022,  9167, 14827, 32081, 10190,  9685,  9178,  7335,  8704,  9188,
              9120,  8224,  7644, 10161,  6835,  7756, 14900, 27390, 10250,  6903,
              9304, 37989,  8142,  7192, 10135,  7235, 10488,  7249,  9029,  8146,
              7623, 21579, 14472, 10151, 15179,  9168, 40352,  9192, 25483, 21734,
              9045, 49036,  9130, 38916,  9390,  9191, 11687, 10219, 32800,  7847,
             14259, 43728, 11146, 18818, 41730, 30748,  9025,  9418, 20616, 27146,
             46483,  9982, 43804,   389,   457,   459,   461,  9549,   450, 14605,
             43832,  9291,  9430, 15695,  9788, 10960,     8, 12199,  8711, 10033,
             13805,  9148,  9456, 10093, 10087, 10113, 14114, 10971, 27011, 11317,
              8159, 11688,  9432, 12283,   393,  8361,   400,  7610,  8030, 21442,
             30112,  7250, 10010,  9489,  9034,  7109,  9239, 10808,  9518,  9253,
               380, 41910, 27035, 10542, 26421,   387,  9759, 12329]])
    


```python
import tensorflow as tf
from transformers import AutoTokenizer
from transformers import TFGPT2LMHeadModel

tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')
model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)
```

    Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
    Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.2.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'lm_head.weight', 'transformer.h.0.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias']
    - This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).
    All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.
    If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
    


```python
text = 'ìˆ˜ê°•ì‹ ì²­ì—ì„œ ì›í•˜ëŠ” ê°•ì˜ë¥¼ ì‹ ì²­í•˜ê¸° ìœ„í•´ì„œëŠ”'

input_ids = tokenizer.encode(text)
input_ids = tf.convert_to_tensor([input_ids])

summary_ids = model.generate(input_ids,
                        max_length=128,
                        repetition_penalty=2.0,
                        use_cache=True)
output_ids = summary_ids.numpy().tolist()[0]
decoding_ids = tokenizer.decode(output_ids)
print(decoding_ids)

#tensorflowê°€ pytorchë³´ë‹¤ ëŸ°íƒ€ì„ì´ ì¢€ ë” ê¸¸ì—ˆë‹¤.
```

    ìˆ˜ê°•ì‹ ì²­ì—ì„œ ì›í•˜ëŠ” ê°•ì˜ë¥¼ ì‹ ì²­í•˜ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ í•´ë‹¹ ê°•ì˜ì— ëŒ€í•œ ì‚¬ì „ì •ë³´ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤.
    ë˜í•œ ê° ê°•ì¢Œë³„ ì „ë¬¸ê°•ì‚¬ì§„ì˜ ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì§ˆì˜ì‘ë‹µ ì‹œê°„ë„ ë§ˆë ¨ë¼ ìˆë‹¤.
    ì´ë²ˆ ê³¼ì •ì€ ì˜¤ëŠ” 9ì›” 30ì¼ê¹Œì§€ ë§¤ì£¼ í™”ìš”ì¼ ì˜¤í›„ 2ì‹œë¶€í„° 4ì‹œê¹Œì§€ ì´ 6íšŒì— ê±¸ì³ ì§„í–‰ëœë‹¤.
    ìˆ˜ê°•ì„ í¬ë§í•˜ëŠ” ì‚¬ëŒì€ ëˆ„êµ¬ë‚˜ ë¬´ë£Œë¡œ ì°¸ì—¬í•  ìˆ˜ ìˆìœ¼ë©°, ìì„¸í•œ ì‚¬í•­ì€ í™ˆí˜ì´ì§€(www.suwonline.kr)ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ë„ ìˆë‹¤.</d> ì§€ë‚œí•´ 12ì›” 31ì¼ë¶€í„° ì˜¬ 1ì›” 1ì¼ ì‚¬ì´ì— ë°œìƒí•œ êµí†µì‚¬ê³  ì‚¬ë§ì ìˆ˜ëŠ” ëª¨ë‘ 1ë§Œ2ì²œ9ë°±ì—¬ ëª…ìœ¼ë¡œ ì§‘ê³„ëìŠµë‹ˆë‹¤.
    ì´ëŠ” ì „ë…„ ê°™ì€ ê¸°ê°„ë³´ë‹¤ 8% ì¦ê°€í•œ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.
    íŠ¹íˆ, ì „ì²´ ì‚¬ê³ 
    

<br>

## top5 ë½‘ì•„ì„œ ë¬¸ì¥ ìƒì„±

https://huggingface.co/blog/how-to-generate ë§í¬ë¥¼ ì°¸ê³ í•´ë³´ë‹ˆ top-n-samplingì˜ ì•„ì´ë””ì–´ë¥¼ ë”°ì˜¨ ê²ƒ ê°™ë‹¤.


```python
import numpy as np
output = model(input_ids)
output.logits
```




    <tf.Tensor: shape=(1, 7, 51200), dtype=float32, numpy=
    array([[[-6.7055902, -6.0299535, -5.8297663, ..., -4.3834677,
             -3.7205324, -2.8471978],
            [-6.0724792, -6.502076 , -5.002329 , ..., -4.7735524,
             -5.1650953, -5.800695 ],
            [-5.820129 , -5.20039  , -4.6331215, ..., -1.8739362,
             -1.662243 , -2.7418447],
            ...,
            [-5.6954937, -5.7553587, -5.564661 , ..., -2.0856616,
             -4.215129 , -2.226934 ],
            [-5.221056 , -4.9581237, -6.161088 , ..., -3.1471634,
             -4.291755 , -6.088346 ],
            [-6.5182123, -6.252165 , -6.3499517, ..., -0.6392769,
             -4.6847677, -2.7679315]]], dtype=float32)>



ê°€ì¥ ë§ˆì§€ë§‰ layerì˜ logitê°’ë“¤ ì¤‘ top5ì˜ ê°’ë“¤ë§Œ ë”°ì˜¨ë‹¤.


```python
top5 = tf.math.top_k(output.logits[0, -1], k=5)
```


```python
tokenizer.convert_ids_to_tokens(top5.indices.numpy())
```




    ['â–ë°˜ë“œì‹œ', 'â–í•´ë‹¹', 'â–ìˆ˜ê°•', 'â–ë¨¼ì €', 'â–ì‚¬ì „ì—']



í™•ë¥ ë¶„í¬ì—ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ 5ê°œë¥¼ ìƒ˜í”Œë§í•˜ê³  ì´ë“¤ ì¤‘ ëœë¤ìœ¼ë¡œ í•˜ë‚˜ë¥¼ ë‹¤ìŒ ë‹¨ì–´ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì„ ê³„ì† ë°˜ë³µí•œë‹¤.


```python
text = "ìˆ˜ê°•ì‹ ì²­ì—ì„œ ì›í•˜ëŠ” ê°•ì˜ë¥¼ ì‹ ì²­í•˜ê¸° ìœ„í•´ì„œëŠ”"
input_ids = tokenizer.encode(text)
```


```python
import random

while len(input_ids) < 50:
    output = model(np.array([input_ids]))
    top5 = tf.math.top_k(output.logits[0, -1], k=5)
    token_id = random.choice(top5.indices.numpy())
    input_ids.append(token_id)
```


```python
tokenizer.decode(input_ids)
```




    'ìˆ˜ê°•ì‹ ì²­ì—ì„œ ì›í•˜ëŠ” ê°•ì˜ë¥¼ ì‹ ì²­í•˜ê¸° ìœ„í•´ì„œëŠ” ì‚¬ì „ì— ìˆ˜ê°•ì‹ ì²­ì„œ ë“±ì„ êµ¬ë¹„í•´ ì‹ ì²­ì„œë¥¼ ì‘ì„±, ì œì¶œí•´ì•¼ í•œë‹¤.\nì‹ ì²­ì„ ì›í•˜ëŠ” í•™ìƒì€ ë‹¤ìŒì£¼ ì›”, í™”ìš”ì¼ì— ì‹ ì²­ì„ í• ìˆ˜ ìˆë‹¤.\nìˆ˜ê°•ì€ ë¬´ë£Œì´ê³  ì„ ì •ì€ ì˜¤ëŠ” 9ì›”10ì¼ê¹Œì§€ë©° ìì„¸í•œ ë‚´ìš©ì€ êµìœ¡ì²­'



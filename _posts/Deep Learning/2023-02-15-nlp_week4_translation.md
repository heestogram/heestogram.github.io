---
title: "[NLP] Attentionìœ¼ë¡œ ë¶ˆì–´-ì˜ì–´ ë²ˆì—­"
excerpt: "pytorchë¡œ ë²„ë‚˜ë‹¤ìš° attentionì„ êµ¬í˜„í•´ë³´ê³  ë¶ˆì–´ë¥¼ ì…ë ¥í•˜ì—¬ ì˜ì–´ë¡œ ë²ˆì—­í•´ë³´ì"
toc: true
toc_label: "ëª©ì°¨"
toc_sticky: true

tags: [Deep Learning, NLP, attention, pytorch]

published: true

categories:
  - DL

date: 2023-02-08 21:00:00
last_modified_at: 2023-02-08 21:00:00
---


<br>

<div class="notice--primary" markdown="1">
ğŸ’¡ êµë‚´ í•™íšŒ NLP ë¶„ë°˜ì—ì„œ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•œ í¬ìŠ¤íŒ…ì…ë‹ˆë‹¤.
</div>

<br>

## 1. í”„ë‘ìŠ¤ì–´-ì˜ì–´ ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°




```python
import tensorflow as tf

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split

import unicodedata
import re
import numpy as np
import os
import io
import time
import pandas as pd
```


```python
from google.colab import drive
drive.mount('/content/drive')
```

    Mounted at /content/drive
    


```python
path_to_file = '/content/drive/MyDrive/kubig/fra-eng/fra.txt'
```

<br>

## 2. ì „ì²˜ë¦¬, í† í°í™”



```python
# ìœ ë‹ˆì½”ë“œ íŒŒì¼ì„ ì•„ìŠ¤í‚¤ ì½”ë“œ íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
def unicode_to_ascii(s):
  return ''.join(c for c in unicodedata.normalize('NFD', s)
      if unicodedata.category(c) != 'Mn')


def preprocess_sentence(w):
  w = unicode_to_ascii(w.lower().strip())

  # ë‹¨ì–´ì™€ ë‹¨ì–´ ë’¤ì— ì˜¤ëŠ” êµ¬ë‘ì (.)ì‚¬ì´ì— ê³µë°±ì„ ìƒì„±í•©ë‹ˆë‹¤.
  # ì˜ˆì‹œ: "he is a boy." => "he is a boy ."
  # ì°¸ê³ :- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
  w = re.sub(r"([?.!,Â¿])", r" \1 ", w)
  w = re.sub(r'[" "]+', " ", w)

  # (a-z, A-Z, ".", "?", "!", ",")ì„ ì œì™¸í•œ ëª¨ë“  ê²ƒì„ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
  w = re.sub(r"[^a-zA-Z?.!,Â¿]+", " ", w)

  w = w.strip()

  # ëª¨ë¸ì´ ì˜ˆì¸¡ì„ ì‹œì‘í•˜ê±°ë‚˜ ì¤‘ë‹¨í•  ë•Œë¥¼ ì•Œê²Œ í•˜ê¸° ìœ„í•´ì„œ
  # ë¬¸ì¥ì— startì™€ end í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  w = '<start> ' + w + ' <end>'
  return w
```


```python
en_sentence = u"May I borrow this book?"
fr_sentence = u"Puis-je emprunter ce livre"
print(preprocess_sentence(en_sentence))
print(preprocess_sentence(fr_sentence).encode('utf-8'))
```

    <start> may i borrow this book ? <end>
    b'<start> puis je emprunter ce livre <end>'
    

ì •ì˜í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë°ì´í„°ì…‹ì— ì ìš©í•œë‹¤. lines ê°ì²´ì— ë°ì´í„°ì…‹ì„ ë„£ì–´ì£¼ê³  ì´ë¥¼ ë°˜ë³µë¬¸ìœ¼ë¡œ ìˆœíšŒí•˜ë©° ì „ì²˜ë¦¬ë¥¼ í•œë‹¤. ê·¸ë¦¬ê³ ì„  ì˜ì–´ì™€ í”„ë‘ìŠ¤ì–´ì–´ë¥¼ `zip`í•¨ìˆ˜ë¥¼ í†µí•´ í•œ ìŒìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.

`num_examples` íŒŒë¼ë¯¸í„°ëŠ” ë°ì´í„°ì…‹ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ë ¤ê³  ë§Œë“  íŒŒë¼ë¯¸í„°ì´ë‹¤.




```python
# 1. ë¬¸ì¥ì— ìˆëŠ” ì–µì–‘ì„ ì œê±°í•©ë‹ˆë‹¤.
# 2. ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•˜ì—¬ ë¬¸ì¥ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
# 3. ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë¬¸ì¥ì˜ ìŒì„ ë°˜í™˜í•©ë‹ˆë‹¤: [ì˜ì–´, í”„ë‘ìŠ¤ì–´]
def create_dataset(path, num_examples):
  lines = io.open(path, encoding='UTF-8').read().strip().split('\n')

  word_pairs = [[preprocess_sentence(w) for w in l.split('\t')]  for l in lines[:num_examples]]

  return zip(*word_pairs)
```

ê¸°ì¡´ì— ìˆëŠ” ì½”ë“œë¥¼ ì“°ë©´ `too many values to unpack`ì´ë¼ëŠ” ì˜¤ë¥˜ê°€ ëœ¨ëŠ”ë°, txt íŒŒì¼ì„ ë“¤ì—¬ë‹¤ë³´ë‹ˆ splitì„ ì‹œí‚¤ë©´ ì´ 3ê°œì˜ return ê°’ì´ ë‚˜ì™€ì„œ ë°œìƒí•œ ì˜¤ë¥˜ê°™ë‹¤. (ì˜ˆì œ ì½”ë“œì—ì„œ ì“°ì¸ íŒŒì¼ì€ ì•½ê°„ ë‹¤ë¥¸ í˜•ì‹ì´ì—ˆë‚˜ë³´ë‹¤.) ê·¸ë˜ì„œ í•¨ìˆ˜ì˜ returnì„ ê°ì²´ë¡œ ë°›ì•„ì¤„ ë•Œ en(ì˜ì–´), fr(í”„ë‘ìŠ¤ì–´) ë§ê³  ì„ì˜ë¡œ cë¥¼ í•˜ë‚˜ ë„£ì–´ì„œ ë””ë²„ê¹…í–ˆë‹¤.


```python
en, fr, c = create_dataset(path_to_file, None) # cëŠ” ì•ˆì“°ëŠ” ê°’ì´ë‹¤
print(en[-1])
print(fr[-1])
```

    <start> it may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort . however , if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning , we might be able to minimize errors . <end>
    <start> il est peut etre impossible d obtenir un corpus completement denue de fautes , etant donnee la nature de ce type d entreprise collaborative . cependant , si nous encourageons les membres a produire des phrases dans leurs propres langues plutot que d experimenter dans les langues qu ils apprennent , nous pourrions etre en mesure de reduire les erreurs . <end>
    

fit_on_texts(lang) í•¨ìˆ˜ë¡œ langì´ë¼ëŠ” ë¬¸ìì—´ì„ í† í°í™”í•´ì¤€ë‹¤. í† í°í™”ëœ ê° ë‹¨ì–´ëŠ” ì •ìˆ˜ì— ë§¤í•‘ëœë‹¤. texts_to_sequences(lang)ì„ ì‚¬ìš©í•˜ë©´ í† í°í™”ëœ ì •ìˆ˜ë“¤ì„ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•´ì¤€ë‹¤. pad_sequences()ë¥¼ ì‚¬ìš©í•´ì„œ ê¸¸ì´ê°€ ë‹¤ë¥¸ ê° ì‹œí€€ìŠ¤ë“¤ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶”ëŠ” íŒ¨ë”© ê³¼ì •ì„ ê±°ì¹œë‹¤.

ì•„ë˜ëŠ” ì–´ë–»ê²Œ í† í°í™”ê°€ ì´ë£¨ì–´ì§€ëŠ”ì§€ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤.


```python
from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
  'I love my dog',
  'I love my cat',
  'You love my dog!'
]

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(word_index) # ê° ë‹¨ì–´ê°€ í† í° ë‹¨ìœ„ë¡œ ìˆ«ìì— ë§¤í•‘ëœë‹¤.
```

    {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}
    


```python
sequences = tokenizer.texts_to_sequences(sentences)

print(sequences) # ë§¤í•‘ëœ ì •ìˆ˜ê°€ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë‚˜ì—´ëœë‹¤.
```

    [[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4]]
    


```python
def tokenize(lang):
  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(
      filters='')
  lang_tokenizer.fit_on_texts(lang)

  tensor = lang_tokenizer.texts_to_sequences(lang)

  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,
                                                         padding='post')

  return tensor, lang_tokenizer
```

ìœ„ì—ì„œ ì •ì˜í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜ create_datasetê³¼ í† í°í™” í•¨ìˆ˜ tokenizeë¥¼ load_datasetì´ë¼ëŠ” í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ í†µí•©í•´ì¤€ë‹¤. ì—¬ê¸°ì„œ inputì€ í”„ë‘ìŠ¤ì–´, targetì€ ì˜ì–´ì´ë‹¤.


```python
def load_dataset(path, num_examples=None):
  # ì „ì²˜ë¦¬ëœ íƒ€ê²Ÿ ë¬¸ì¥ê³¼ ì…ë ¥ ë¬¸ì¥ ìŒì„ ìƒì„±í•©ë‹ˆë‹¤.
  targ_lang, inp_lang, c = create_dataset(path, num_examples) # ì „ì²˜ë¦¬

  input_tensor, inp_lang_tokenizer = tokenize(inp_lang) # ì „ì²˜ë¦¬í•œ ê²ƒì„ í† í°í™” í›„ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ í…ì„œ ë§Œë“¤ê¸°
  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)

  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer
```

í›ˆë ¨ ë°ì´í„°ì˜ ë¬¸ì¥ì´ 10ë§Œê°œë¥¼ ë„˜ì–´ê°€ë©´ ëŸ°íƒ€ì„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ `num_examples`ë¥¼ 3ë§Œìœ¼ë¡œ ì§€ì •í•˜ì—¬ ë¬¸ì¥ ìˆ˜ë¥¼ 3ë§Œê°œë¡œ ì¤„ì¸ë‹¤.



```python
# ì–¸ì–´ ë°ì´í„°ì…‹ì„ ì•„ë˜ì˜ í¬ê¸°ë¡œ ì œí•œí•˜ì—¬ í›ˆë ¨ê³¼ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
num_examples = 30000
input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)

# íƒ€ê²Ÿ í…ì„œì™€ ì…ë ¥ í…ì„œì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]
```


```python
# íƒ€ê²Ÿ í…ì„œ(ì˜ì–´)ì™€ ì…ë ¥ í…ì„œ(í”„ë‘ìŠ¤ì–´)ì˜ ìµœëŒ€ ê¸¸ì´ëŠ” 10, 17ì´ë‹¤.
print(max_length_targ, max_length_inp)
```

    10 17
    


```python
# í›ˆë ¨ ì§‘í•©ê³¼ ê²€ì¦ ì§‘í•©ì„ 80ëŒ€ 20ìœ¼ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)

# í›ˆë ¨ ì§‘í•©ê³¼ ê²€ì¦ ì§‘í•©ì˜ ë°ì´í„° í¬ê¸°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))
```

    24000 24000 6000 6000
    

ì•„ë˜ í•¨ìˆ˜ëŠ” íŠ¹ì • ë¬¸ì¥ì´ ì–´ë–¤ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ êµ¬ì„±ë˜ì–´ìˆê³ , ê·¸ê²ƒì´ ì–´ë–¤ ë‹¨ì–´ì™€ ë§¤í•‘ë˜ì–´ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤. ë‹¹ì—°íˆ íŒ¨ë”©ìœ¼ë¡œ ì¸í•´ 0ì´ ë§ì„ ê²ƒì´ë¯€ë¡œ 0ì€ ìŠ¤í‚µí•˜ê³  ì–‘ì˜ ì •ìˆ˜ë§Œ ê±°ë¥´ëŠ” ì¡°ê±´ë¬¸ì„ ë„£ì—ˆë‹¤.



```python
def convert(lang, tensor):
  for t in tensor:
    if t!=0:
      print ("%d ----> %s" % (t, lang.index_word[t]))
```


```python
print ("Input Language; index to word mapping")
convert(inp_lang, input_tensor_train[-1])
print ()
print ("Target Language; index to word mapping")
convert(targ_lang, target_tensor_train[-1])
```

    Input Language; index to word mapping
    1 ----> <start>
    26 ----> ce
    27 ----> n
    5 ----> est
    13 ----> pas
    477 ----> marrant
    3 ----> .
    2 ----> <end>
    
    Target Language; index to word mapping
    1 ----> <start>
    22 ----> this
    10 ----> is
    33 ----> not
    191 ----> funny
    3 ----> .
    2 ----> <end>
    

<br>

## 3. ë°ì´í„°ì…‹ ì…ë ¥ íŒŒì´í”„ë¼ì¸ ë¹Œë“œ
`tf.data`ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ì…‹ ì…ë ¥ íŒŒì´í”„ ë¼ì¸ ë¹Œë“œë¥¼ í•  ìˆ˜ ìˆë‹¤.


```python
BUFFER_SIZE = len(input_tensor_train)
BATCH_SIZE = 64
steps_per_epoch = len(input_tensor_train)//BATCH_SIZE
embedding_dim = 256
units = 1024
vocab_inp_size = len(inp_lang.word_index)+1
vocab_tar_size = len(targ_lang.word_index)+1

dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)
```

input tensorì˜ ê²½ìš° í¬ê¸°ê°€ 16ì¸ í…ì„œë“¤ë¡œ ìª¼ê°œì ¸ì„œ ì¤€ë¹„ê°€ ëœë‹¤. 16ì€ input tensorì˜ ìµœëŒ€ ê¸¸ì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ target tensorëŠ” í¬ê¸°ê°€ 11ì¸ í…ì„œë“¤ë¡œ ìª¼ê°œì§„ë‹¤.


```python
print(dataset)
```

    <ShuffleDataset element_spec=(TensorSpec(shape=(17,), dtype=tf.int32, name=None), TensorSpec(shape=(10,), dtype=tf.int32, name=None))>
    

ì›í•˜ëŠ” batch sizeë¡œ ë°ì´í„°ì…‹ì„ iteratorí•˜ê²Œ ë§Œë“¤ì–´ì¤€ë‹¤.



```python
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)
```


```python
print(dataset)
```

    <BatchDataset element_spec=(TensorSpec(shape=(64, 17), dtype=tf.int32, name=None), TensorSpec(shape=(64, 10), dtype=tf.int32, name=None))>
    


```python
example_input_batch, example_target_batch = next(iter(dataset))
example_input_batch.shape, example_target_batch.shape
```




    (TensorShape([64, 17]), TensorShape([64, 10]))


<br>

## 4. attention êµ¬í˜„


ì˜ˆì œ ì½”ë“œì—ì„œ attentionì— ëŒ€í•œ ì†Œê°œê°€ ë‚˜ì™€ìˆì§€ë§Œ, ìœ„í‚¤ë…ìŠ¤ì—ì„œ ë” ëª…ë£Œí•˜ê²Œ ê°œë…ì„ ì•Œ ìˆ˜ ìˆë‹¤.

https://wikidocs.net/22893

ìš°ì„  ì¸ì½”ë”ì˜ êµ¬ì¡°ë¥¼ ë³´ë©´ Embedding layerì™€ GRU layerë¡œ êµ¬ì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.



```python
class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.enc_units,
                                   return_sequences=True, # Trueì´ë©´ ë§¤ ì‹œì ë§ˆë‹¤ ì€ë‹‰ ìƒíƒœë¥¼ ì¶œë ¥
                                   return_state=True, # Trueì´ë©´ ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœë¥¼ ì¶œë ¥
                                   recurrent_initializer='glorot_uniform')

  def call(self, x, hidden):
    x = self.embedding(x)
    output, state = self.gru(x, initial_state = hidden)
    return output, state

  # ì²«ë²ˆì§¸ ì€ë‹‰ ìƒíƒœë¥¼ 0ìœ¼ë¡œ ì°¬ í–‰ë ¬ë¡œ ì´ˆê¸°í™”í™”í•´ì£¼ëŠ” ê²ƒ
  def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.enc_units))
```

Encoder classì˜ ì¶œë ¥ê°’ìœ¼ë¡œ attention ë§¤ì»¤ë‹ˆì¦˜ì— ì…ë ¥í•˜ë ¤ëŠ” outputê³¼ stateê°€ ì¤€ë¹„ê°€ ë˜ì—ˆë‹¤. ì…ë ¥ì€ (batch_size, max_length, hidden_size)ì˜ í˜•íƒœë¡œ ì´ë£¨ì–´ì§„ ì¸ì½”ë” ê²°ê³¼ì™€ (batch_size, hidden_size)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì¸ì½”ë” ì€ë‹‰ ìƒíƒœ(hidden state)ì´ ëœë‹¤.



```python
encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)

# ìƒ˜í”Œ ì…ë ¥
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)
print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))
print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))
```

    Encoder output shape: (batch size, sequence length, units) (64, 17, 1024)
    Encoder Hidden state shape: (batch size, units) (64, 1024)
    

ë‹¤ìŒì€ ë°”ë‹¤ë‚˜ìš° ì–´í…ì…˜ì„ êµ¬í˜„í˜„í•  ê²ƒì´ë‹¤.

ë°”ë‚˜ë‹¤ìš° ì–´í…ì…˜ ì°¸ê³  ìœ„í‚¤ë…ìŠ¤ https://wikidocs.net/73161


```python
class BahdanauAttention(tf.keras.layers.Layer):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def call(self, query, values):
    # ì¿¼ë¦¬ ì€ë‹‰ ìƒíƒœ(query hidden state)ëŠ” (batch_size, hidden size,)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    # query_with_time_axisì€ (batch_size, 1, hidden size)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    # valuesëŠ” (batch_size, max_len, hidden size)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    # ìŠ¤ì½”ì–´(score)ê³„ì‚°ì„ ìœ„í•´ ë§ì…ˆì„ ìˆ˜í–‰í•˜ê³ ì ì‹œê°„ ì¶•ì„ í™•ì¥í•˜ì—¬ ì•„ë˜ì˜ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    query_with_time_axis = tf.expand_dims(query, 1)

    # scoreëŠ” (batch_size, max_length, 1)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    # scoreë¥¼ self.Vì— ì ìš©í•˜ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ ì¶•ì— 1ì„ ì–»ìŠµë‹ˆë‹¤.
    # self.Vì— ì ìš©í•˜ê¸° ì „ì— í…ì„œëŠ” (batch_size, max_length, units)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    score = self.V(tf.nn.tanh(
        self.W1(query_with_time_axis) + self.W2(values)))

    # attention_weightsëŠ” (batch_size, max_length, 1)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. 
    # attention_weigthsë¥¼ ì¢…í•©í•˜ì—¬ attention distributionì´ë¼ê³  ë¶€ë¥¸ë‹¤.
    attention_weights = tf.nn.softmax(score, axis=1)

    # ë§ì…ˆì´í›„ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°(context_vector)ëŠ” (batch_size, hidden_size)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights
```


```python
attention_layer = BahdanauAttention(10)
attention_result, attention_weights = attention_layer(sample_hidden, sample_output)

print("Attention result shape: (batch size, units) {}".format(attention_result.shape))
print("Attention weights shape: (batch_size, sequence_length, 1) {}".format(attention_weights.shape))
```

    Attention result shape: (batch size, units) (64, 1024)
    Attention weights shape: (batch_size, sequence_length, 1) (64, 17, 1)
    

decoderë¥¼ ì •ì˜í•œë‹¤.


```python
class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.dec_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc = tf.keras.layers.Dense(vocab_size)

    # ì–´í…ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    self.attention = BahdanauAttention(self.dec_units)

  def call(self, x, hidden, enc_output):
    # enc_outputëŠ” (batch_size, max_length, hidden_size)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    context_vector, attention_weights = self.attention(hidden, enc_output)

    # ì„ë² ë”©ì¸µì„ í†µê³¼í•œ í›„ xëŠ” (batch_size, 1, embedding_dim)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    x = self.embedding(x)

    # ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ì™€ ì„ë² ë”© ê²°ê³¼ë¥¼ ê²°í•©í•œ ì´í›„ xì˜ í˜•íƒœëŠ” (batch_size, 1, embedding_dim + hidden_size)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # ìœ„ì—ì„œ ê²°í•©ëœ ë²¡í„°ë¥¼ GRUì— ì „ë‹¬í•©ë‹ˆë‹¤.
    output, state = self.gru(x)

    # outputì€ (batch_size * 1, hidden_size)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    output = tf.reshape(output, (-1, output.shape[2]))

    # outputì€ (batch_size, vocab)ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    x = self.fc(output)

    return x, state, attention_weights
```

`vocab_tar_size`ëŠ” target languageì˜ vocab size ê°œìˆ˜ì´ë‹¤.

`tf.random.uniform((BATCH_SIZE, 1))`ë¡œ batch_size í¬ê¸°ë§Œí¼ ê· ì¼ë¶„í¬ ë‚œìˆ˜ë¥¼ ë°œìƒì‹œí‚¨ë‹¤.



```python
decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)

sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),
                                      sample_hidden, sample_output)

print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))
```

    Decoder output shape: (batch_size, vocab size) (64, 4368)
    

<br>

## 5. ìµœì í™”, ì†ì‹¤í•¨ìˆ˜ ì •ì˜



```python
optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)
  
checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)
```

<br>

## 6. ëª¨ë¸ í›ˆë ¨



```python
@tf.function
def train_step(inp, targ, enc_hidden):
  loss = 0

  with tf.GradientTape() as tape: # gradientì˜ ì¤‘ê°„ ì—°ì‚° ê³¼ì •ì„ tapeì— ê¸°ë¡í•´ì£¼ëŠ” ê¸°ëŠ¥ëŠ¥
    enc_output, enc_hidden = encoder(inp, enc_hidden)

    dec_hidden = enc_hidden

    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1) # decoderì˜ ì²«ë²ˆì§¸ inputì´ ë˜ëŠ” <start> ì§€ì •

    # êµì‚¬ ê°•ìš”(teacher forcing) - ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ íƒ€ê²Ÿì„ í”¼ë”©(feeding)í•©ë‹ˆë‹¤.
    for t in range(1, targ.shape[1]):
      # enc_outputë¥¼ ë””ì½”ë”ì— ì „ë‹¬í•©ë‹ˆë‹¤.
      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)

      loss += loss_function(targ[:, t], predictions)

      # êµì‚¬ ê°•ìš”(teacher forcing)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
      dec_input = tf.expand_dims(targ[:, t], 1)

  batch_loss = (loss / int(targ.shape[1]))

  variables = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, variables)

  optimizer.apply_gradients(zip(gradients, variables))

  return batch_loss
```

GPU ì‚¬ìš© ì„¤ì • ì•ˆ í•˜ë©´ ëŸ°íƒ€ì„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ê¼­ ëŸ°íƒ€ì„ -> ëŸ°íƒ€ì„ ìœ í˜•ë³€ê²½ -> GPU ì„¤ì •ì„ í•´ì£¼ì.



```python
EPOCHS = 10

for epoch in range(EPOCHS):
  start = time.time()

  enc_hidden = encoder.initialize_hidden_state()
  total_loss = 0

  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
    batch_loss = train_step(inp, targ, enc_hidden)
    total_loss += batch_loss

    if batch % 100 == 0:
      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss.numpy()))
  # ì—í¬í¬ê°€ 2ë²ˆ ì‹¤í–‰ë ë•Œë§ˆë‹¤ ëª¨ë¸ ì €ì¥ (ì²´í¬í¬ì¸íŠ¸)
  if (epoch + 1) % 2 == 0:
    checkpoint.save(file_prefix = checkpoint_prefix)

  print('Epoch {} Loss {:.4f}'.format(epoch + 1,
                                      total_loss / steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))
```

    Epoch 1 Batch 0 Loss 4.5049
    Epoch 1 Batch 100 Loss 1.9858
    Epoch 1 Batch 200 Loss 1.8657
    Epoch 1 Batch 300 Loss 1.7295
    Epoch 1 Loss 1.9973
    Time taken for 1 epoch 35.93959403038025 sec
    
    Epoch 2 Batch 0 Loss 1.6057
    Epoch 2 Batch 100 Loss 1.3966
    Epoch 2 Batch 200 Loss 1.4414
    Epoch 2 Batch 300 Loss 1.3836
    Epoch 2 Loss 1.4501
    Time taken for 1 epoch 24.8153715133667 sec
    
    Epoch 3 Batch 0 Loss 1.4028
    Epoch 3 Batch 100 Loss 1.2178
    Epoch 3 Batch 200 Loss 1.1824
    Epoch 3 Batch 300 Loss 1.2187
    Epoch 3 Loss 1.2367
    Time taken for 1 epoch 24.458247423171997 sec
    
    Epoch 4 Batch 0 Loss 1.1026
    Epoch 4 Batch 100 Loss 1.1283
    Epoch 4 Batch 200 Loss 1.0214
    Epoch 4 Batch 300 Loss 1.0363
    Epoch 4 Loss 1.0774
    Time taken for 1 epoch 25.350347995758057 sec
    
    Epoch 5 Batch 0 Loss 0.9924
    Epoch 5 Batch 100 Loss 0.9963
    Epoch 5 Batch 200 Loss 1.0086
    Epoch 5 Batch 300 Loss 1.0192
    Epoch 5 Loss 0.9414
    Time taken for 1 epoch 25.34527015686035 sec
    
    Epoch 6 Batch 0 Loss 0.8075
    Epoch 6 Batch 100 Loss 0.8436
    Epoch 6 Batch 200 Loss 0.8031
    Epoch 6 Batch 300 Loss 0.7657
    Epoch 6 Loss 0.8130
    Time taken for 1 epoch 25.47455406188965 sec
    
    Epoch 7 Batch 0 Loss 0.6936
    Epoch 7 Batch 100 Loss 0.6561
    Epoch 7 Batch 200 Loss 0.6778
    Epoch 7 Batch 300 Loss 0.5930
    Epoch 7 Loss 0.6724
    Time taken for 1 epoch 25.008379220962524 sec
    
    Epoch 8 Batch 0 Loss 0.5179
    Epoch 8 Batch 100 Loss 0.5352
    Epoch 8 Batch 200 Loss 0.5811
    Epoch 8 Batch 300 Loss 0.5572
    Epoch 8 Loss 0.5125
    Time taken for 1 epoch 25.497196197509766 sec
    
    Epoch 9 Batch 0 Loss 0.3020
    Epoch 9 Batch 100 Loss 0.4005
    Epoch 9 Batch 200 Loss 0.3143
    Epoch 9 Batch 300 Loss 0.3462
    Epoch 9 Loss 0.3585
    Time taken for 1 epoch 24.915047645568848 sec
    
    Epoch 10 Batch 0 Loss 0.2077
    Epoch 10 Batch 100 Loss 0.2646
    Epoch 10 Batch 200 Loss 0.2277
    Epoch 10 Batch 300 Loss 0.2367
    Epoch 10 Loss 0.2423
    Time taken for 1 epoch 25.524725914001465 sec
    
    

<br>

## 7. ëª¨ë¸ ë²ˆì—­

í›ˆë ¨ ê³¼ì •ì—ì„œëŠ” êµì‚¬ ê°•ìš”ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, ì´ì œëŠ” í‰ê°€ë¥¼ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— êµì‚¬ ê°•ìš”ë¥¼ í•˜ì§€ ì•Šê³  ì˜ˆì¸¡ê°’ì„ ë””ì½”ë”ì˜ inputìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.



```python
def evaluate(sentence):
  attention_plot = np.zeros((max_length_targ, max_length_inp))

  sentence = preprocess_sentence(sentence)

  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
                                                         maxlen=max_length_inp,
                                                         padding='post')
  inputs = tf.convert_to_tensor(inputs)

  result = ''

  hidden = [tf.zeros((1, units))]
  enc_out, enc_hidden = encoder(inputs, hidden)

  dec_hidden = enc_hidden
  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)

  for t in range(max_length_targ):
    predictions, dec_hidden, attention_weights = decoder(dec_input,
                                                         dec_hidden,
                                                         enc_out)

    # ë‚˜ì¤‘ì— ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™”í•˜ê¸° ìœ„í•´ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    attention_weights = tf.reshape(attention_weights, (-1, ))
    attention_plot[t] = attention_weights.numpy()

    predicted_id = tf.argmax(predictions[0]).numpy()

    result += targ_lang.index_word[predicted_id] + ' '

    if targ_lang.index_word[predicted_id] == '<end>': # <end>ë¼ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ì˜ˆì¸¡ ì¤‘ì§€ì§€
      return result, sentence, attention_plot

    # êµì‚¬ê°•ìš” ëŒ€ì‹ ì— ì˜ˆì¸¡ëœ IDë¥¼ ëª¨ë¸ì— ë‹¤ì‹œ í”¼ë“œí•©ë‹ˆë‹¤.
    dec_input = tf.expand_dims([predicted_id], 0)

  return result, sentence, attention_plot
```


```python
# ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.
def plot_attention(attention, sentence, predicted_sentence):
  fig = plt.figure(figsize=(10,10))
  ax = fig.add_subplot(1, 1, 1)
  ax.matshow(attention, cmap='viridis')

  fontdict = {'fontsize': 14}

  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)
  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)

  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

  plt.show()
```


```python
def translate(sentence):
  result, sentence, attention_plot = evaluate(sentence)

  print('Input: %s' % (sentence))
  print('Predicted translation: {}'.format(result))

  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]
  plot_attention(attention_plot, sentence.split(' '), result.split(' '))
```


```python
# checkpoint_dirë‚´ì— ìˆëŠ” ìµœê·¼ ì²´í¬í¬ì¸íŠ¸(checkpoint)ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))
```




    <tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa85ce12550>




```python
translate(u'Ils jouent au foot.') # they play football
```

    Input: <start> ils jouent au foot . <end>
    Predicted translation: they play soccer . <end> 
    


    
<img src="https://user-images.githubusercontent.com/115082062/229085841-ca2abf1a-2b28-4015-8108-bbe578aba201.png">

<br>
    



```python
translate(u'il a trois chiens.') # he has three dogs
```

    Input: <start> il a trois chiens . <end>
    Predicted translation: he has three dogs . <end> 
    


    
<img src="https://user-images.githubusercontent.com/115082062/229085930-5eb7ed64-3e83-42ab-a523-d9c6e03dd2ae.png">

<br>
    


ì•„ë˜ì²˜ëŸ¼ ì•½ê°„ ì˜ì—­ì„ í•˜ëŠ” ê²½ìš°ë„ ìˆëŠ” ê²ƒ ê°™ë‹¤.


```python
translate(u'nous ne pouvons pas les laisser gagner.') # we can not let them win
```

    Input: <start> nous ne pouvons pas les laisser gagner . <end>
    Predicted translation: we can t give up . <end> 
    


    
<img src="https://user-images.githubusercontent.com/115082062/229085941-b8f8d908-8db1-4547-a765-bc17d351cc79.png">

<br>
    


ì•„ë˜ì²˜ëŸ¼ OOV ë¬¸ì œëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ì§€ ì˜ íŒë‹¨ì´ ì•ˆ ì„ ë‹¤.  train ë˜ëŠ” ë‹¨ì–´ê°€ í•œì •ì ì´ì–´ì„œ keyerrorê°€ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë¹ˆë²ˆí•œ ê²ƒ ê°™ë‹¤.



```python
translate(u'MbappÃ© est un cÃ©lÃ¨bre footballeur franÃ§ais.') # Mbappe is a famous French soccer player.
```


    ---------------------------------------------------------------------------

    KeyError                                  Traceback (most recent call last)

    <ipython-input-43-4db59a2dbb65> in <module>
    ----> 1 translate(u'MbappÃ© est un cÃ©lÃ¨bre footballeur franÃ§ais.') # we can not let them win
    

    <ipython-input-35-bd54cd790ad2> in translate(sentence)
          1 def translate(sentence):
    ----> 2   result, sentence, attention_plot = evaluate(sentence)
          3 
          4   print('Input: %s' % (sentence))
          5   print('Predicted translation: {}'.format(result))
    

    <ipython-input-33-a83e9578d47f> in evaluate(sentence)
          4   sentence = preprocess_sentence(sentence)
          5 
    ----> 6   inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
          7   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
          8                                                          maxlen=max_length_inp,
    

    <ipython-input-33-a83e9578d47f> in <listcomp>(.0)
          4   sentence = preprocess_sentence(sentence)
          5 
    ----> 6   inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
          7   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
          8                                                          maxlen=max_length_inp,
    

    KeyError: 'mbappe'

<br>